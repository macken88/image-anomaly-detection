{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "72ef0a38",
      "metadata": {},
      "source": [
        "# 02_evaluation_report — カテゴリ横断評価（Mahalanobis / PaDiM）\n",
        "\n",
        "本ノートでは、指定カテゴリの MVTec AD を対象に、Mahalanobis 距離ベースと PaDiM の2手法で精度を比較します。\n",
        "\n",
        "評価ポリシー:\n",
        "- 共分散（およびPaDiM統計）の推定は、各カテゴリの train/good 全画像を使用（CVは行わない）\n",
        "- モデルの精度評価は test データで実施\n",
        "- 評価カテゴリはリストで与え、順に推論を実行（初期値: leather, tile, wood, bottle）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d01d99a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 依存ライブラリの読み込み\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime, timezone\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import warnings\n",
        "\n",
        "# sklearn が無い環境でも動作するよう、読み込みはベストエフォート\n",
        "try:\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
        "except Exception as e:\n",
        "    roc_auc_score = average_precision_score = f1_score = None\n",
        "    print('[WARN] sklearn.metrics を読み込めませんでした。一部指標が計算できない可能性があります。', e)\n",
        "\n",
        "# anomaly_detectors からコア関数をインポート\n",
        "import importlib, anomaly_detectors\n",
        "importlib.invalidate_caches(); importlib.reload(anomaly_detectors)\n",
        "from anomaly_detectors import (\n",
        "    fit_mahalanobis, all_mahalanobis_scores,\n",
        "    fit_padim, padim_heatmap, all_padim_scores,\n",
        ")\n",
        "\n",
        "RESULTS = Path('runs'); RESULTS.mkdir(parents=True, exist_ok=True)\n",
        "MVTEC_ROOT = Path(os.environ.get('MVTEC_ROOT', 'data/mvtec'))\n",
        "MVTEC_ROOT.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1a86f1b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# torchvision のモデル名から学習済みモデルを読み込む関数\n",
        "def load_backbone_from_name(name: str):\n",
        "    try:\n",
        "        from torchvision.models import get_model, get_model_weights\n",
        "        weights = None\n",
        "        try:\n",
        "            weights = get_model_weights(name).DEFAULT  # 学習済みウェイト\n",
        "        except Exception:\n",
        "            pass  # ウェイトが無いモデルはランダム初期化で作る\n",
        "        return get_model(name, weights=weights).eval()\n",
        "    except Exception:\n",
        "        # 旧 API フォールバック（古い torchvision 向け）\n",
        "        if not hasattr(models, name):\n",
        "            raise ValueError(f\"Unknown backbone name: {name}\")\n",
        "        return models.__dict__[name](pretrained=True).eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2fb9835",
      "metadata": {},
      "outputs": [],
      "source": [
        "# JSONからパラメータ読み込み\n",
        "with open(\"assets/fixed_pipeline.json\") as f:\n",
        "    cfg = json.load(f)\n",
        "\n",
        "backbone = cfg['common']['backbone']\n",
        "model = load_backbone_from_name(backbone)\n",
        "image_size = cfg['common']['image_size']\n",
        "threshold_percentile = cfg['common']['threshold_percentile']\n",
        "MD_layer = cfg['mahalanobis']['layer']\n",
        "padim_layers = cfg['padim']['layers']\n",
        "padim_channel_subsample = cfg['padim']['d']\n",
        "\n",
        "# 各種設定\n",
        "num_workers = 0 #min(4, os.cpu_count() or 1)\n",
        "batch_size = 32\n",
        "eval_categories = ['tile']#, 'lether', 'wood', 'bottle', 'cable', 'capsule']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323c8392",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1545ebdf",
      "metadata": {},
      "source": [
        "## データユーティリティ（ノートブック内に保持）\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ee0f81cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 変換（ImageNet の統計を使用）\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "])\n",
        "\n",
        "class ImagePathDataset(Dataset):\n",
        "    \"\"\"最小限の画像データセット。 (tensor, label) を返す。\n",
        "\n",
        "    Args:\n",
        "        paths (List[Path]): 画像パス群\n",
        "        labels (List[Any]): 同長のラベル\n",
        "        transform: 前処理変換\n",
        "    \"\"\"\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = [Path(p) for p in paths]\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]; y = self.labels[idx]\n",
        "        img = Image.open(p).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, y\n",
        "\n",
        "from typing import List, Tuple, Any\n",
        "def _existing_category_root(category: str) -> Path:\n",
        "    candidates = [\n",
        "        MVTEC_ROOT / category,\n",
        "        Path('datasets/MVTecAD') / category,\n",
        "        Path('MVtec_dataset') / category,\n",
        "    ]\n",
        "    for c in candidates:\n",
        "        if c.exists():\n",
        "            return c\n",
        "    raise FileNotFoundError(f'カテゴリが見つかりません: {category}')\n",
        "\n",
        "def _list_images(d: Path) -> List[Path]:\n",
        "    exts = {'.png', '.jpg', '.jpeg'}\n",
        "    if not d.exists():\n",
        "        return []\n",
        "    return sorted([p for p in d.rglob('*') if p.suffix.lower() in exts])\n",
        "\n",
        "def build_train_and_test_loaders(category: str, batch_size: int = 32) -> Tuple[DataLoader, DataLoader]:\n",
        "    root = _existing_category_root(category)\n",
        "    train_good = _list_images(root / 'train' / 'good')\n",
        "    assert len(train_good) > 0, f'No train/good images: {category}'\n",
        "    # train: 全 good を使用\n",
        "    train_ds = ImagePathDataset(train_good, [0]*len(train_good), transform=_transform)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "    # test: サブディレクトリ名をラベルとして使用（'good'以外は異常）\n",
        "    test_dir = root / 'test'\n",
        "    test_paths, test_labels = [], []\n",
        "    for sub in sorted([d for d in test_dir.iterdir() if d.is_dir()], key=lambda p: p.name):\n",
        "        label = sub.name\n",
        "        paths = _list_images(sub)\n",
        "        if paths:\n",
        "            test_paths.extend(paths)\n",
        "            test_labels.extend([label]*len(paths))\n",
        "    assert len(test_paths) > 0, f'No test images: {category}'\n",
        "    test_ds = ImagePathDataset(test_paths, test_labels, transform=_transform)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f05b9fc",
      "metadata": {},
      "source": [
        "## 指標計算ユーティリティ\n",
        "- 閾値は train/good のスコア分布から FPR=1% を満たす分位点で設定\n",
        "- AUROC/AUPRC はしきい値に依らないため、test スコアと真値で計算\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3082333a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def fpr_threshold_from_neg(scores_neg: np.ndarray, fpr: float = 0.01) -> float:\n",
        "    \"\"\"負例（正常: train/good）のスコアから、指定 FPR を達成するしきい値を返す。\n",
        "    右裾（スコアが高いほど異常）を閾切りするため、(1 - fpr) 分位点を採用。\n",
        "    \"\"\"\n",
        "    fpr = float(np.clip(fpr, 1e-6, 1-1e-6))\n",
        "    return float(np.quantile(scores_neg, 1.0 - fpr))\n",
        "\n",
        "def to_binary_labels(labels: list) -> np.ndarray:\n",
        "    # 'good' を 0、その他（欠陥名）を 1 とする\n",
        "    return np.array([0 if str(y) == 'good' else 1 for y in labels], dtype=np.int64)\n",
        "\n",
        "def compute_metrics(y_true: np.ndarray, y_score: np.ndarray, y_pred: np.ndarray) -> dict:\n",
        "    out = {}\n",
        "    if roc_auc_score is not None:\n",
        "        try:\n",
        "            out['auroc'] = float(roc_auc_score(y_true, y_score))\n",
        "        except Exception as e:\n",
        "            out['auroc'] = None\n",
        "    if average_precision_score is not None:\n",
        "        try:\n",
        "            out['auprc'] = float(average_precision_score(y_true, y_score))\n",
        "        except Exception as e:\n",
        "            out['auprc'] = None\n",
        "    if f1_score is not None:\n",
        "        try:\n",
        "            out['f1'] = float(f1_score(y_true, y_pred))\n",
        "        except Exception as e:\n",
        "            out['f1'] = None\n",
        "    out['acc'] = float((y_true == y_pred).mean())\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3532e99",
      "metadata": {},
      "source": [
        "## カテゴリ評価関数（Mahalanobis / PaDiM）\n",
        "- 各カテゴリで学習（train/good 全使用）→ test 評価\n",
        "- しきい値は train/good の FPR=1% を満たすように設定\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7d36e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_category(category: str, backbone: str) -> dict:\n",
        "    # データローダー用意\n",
        "    train_loader, test_loader = build_train_and_test_loaders(category, batch_size=batch_size)\n",
        "\n",
        "    # test の真値（2値）\n",
        "    y_true = to_binary_labels(test_loader.dataset.labels)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # ---- Mahalanobis ----\n",
        "    print('start Mahalanobis fitting...')\n",
        "    state_m = fit_mahalanobis(train_loader, model, feature_node=MD_layer, device=device)\n",
        "    scores_train_m = all_mahalanobis_scores(state_m, train_loader).numpy()\n",
        "    #thr_m = fpr_threshold_from_neg(scores_train_m, fpr=fpr_target)\n",
        "    thr_m = np.percentile(scores_train_m, threshold_percentile)\n",
        "    scores_test_m = all_mahalanobis_scores(state_m, test_loader).numpy()\n",
        "    y_pred_m = (scores_test_m >= thr_m).astype(np.int64)\n",
        "    results['mahalanobis'] = {\n",
        "        'threshold': float(thr_m),\n",
        "        **compute_metrics(y_true, scores_test_m, y_pred_m),\n",
        "    }\n",
        "\n",
        "    # ---- PaDiM ----\n",
        "    print('start PaDiM fitting...')\n",
        "    state_p = fit_padim(\n",
        "        train_loader,\n",
        "        model,\n",
        "        layers=padim_layers,\n",
        "        d=padim_channel_subsample,\n",
        "        device=device\n",
        "    )\n",
        "    scores_train_p = all_padim_scores(state_p, train_loader).numpy()\n",
        "    #thr_p = fpr_threshold_from_neg(scores_train_p, fpr=fpr_target)\n",
        "    thr_p = np.percentile(scores_train_p, threshold_percentile)\n",
        "    scores_test_p = all_padim_scores(state_p, test_loader).numpy()\n",
        "    y_pred_p = (scores_test_p >= thr_p).astype(np.int64)\n",
        "    results['padim'] = {\n",
        "        'threshold': float(thr_p),\n",
        "        **compute_metrics(y_true, scores_test_p, y_pred_p),\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        'category': category,\n",
        "        'backbone': backbone,\n",
        "        'results': results,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f19add1",
      "metadata": {},
      "source": [
        "## 実行（カテゴリ一括）\n",
        "- 結果は runs/eval/<category>/metrics_{method}.json にも保存します。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "718dee77",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[RUN] category=tile, backbone=resnet18\n",
            "start Mahalanobis fitting...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sim_m/work/image-anomaly-detection/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cat \u001b[38;5;129;01min\u001b[39;00m eval_categories:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[RUN] category=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, backbone=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackbone\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     out = \u001b[43mevaluate_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mcompleted.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m     ts = datetime.now(timezone.utc).isoformat()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mevaluate_category\u001b[39m\u001b[34m(category, backbone)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# ---- Mahalanobis ----\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mstart Mahalanobis fitting...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m state_m = \u001b[43mfit_mahalanobis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_node\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMD_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m scores_train_m = all_mahalanobis_scores(state_m, train_loader).numpy()\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#thr_m = fpr_threshold_from_neg(scores_train_m, fpr=fpr_target)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/image-anomaly-detection/anomaly_detectors/mahalanobis.py:109\u001b[39m, in \u001b[36mfit_mahalanobis\u001b[39m\u001b[34m(train_loader, backbone, feature_node, cov_estimator, reg_eps, device)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# 共分散推定\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cov_estimator == \u001b[33m\"\u001b[39m\u001b[33mledoit_wolf\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _HAS_SKLEARN:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     cov, _ = \u001b[43m_ledoit_wolf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;66;03m# 標本共分散（1/(N-1) スケール）。sklearn が無い場合のフォールバック。\u001b[39;00m\n\u001b[32m    112\u001b[39m     cov = np.cov(feats, rowvar=\u001b[38;5;28;01mFalse\u001b[39;00m, bias=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/image-anomaly-detection/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/image-anomaly-detection/.venv/lib/python3.12/site-packages/sklearn/covariance/_shrunk_covariance.py:462\u001b[39m, in \u001b[36mledoit_wolf\u001b[39m\u001b[34m(X, assume_centered, block_size)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[32m    404\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33marray-like\u001b[39m\u001b[33m\"\u001b[39m]},\n\u001b[32m    405\u001b[39m     prefer_skip_nested_validation=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    406\u001b[39m )\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mledoit_wolf\u001b[39m(X, *, assume_centered=\u001b[38;5;28;01mFalse\u001b[39;00m, block_size=\u001b[32m1000\u001b[39m):\n\u001b[32m    408\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Estimate the shrunk Ledoit-Wolf covariance matrix.\u001b[39;00m\n\u001b[32m    409\u001b[39m \n\u001b[32m    410\u001b[39m \u001b[33;03m    Read more in the :ref:`User Guide <shrunk_covariance>`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m \u001b[33;03m    np.float64(0.23)\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    458\u001b[39m     estimator = \u001b[43mLedoitWolf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m        \u001b[49m\u001b[43massume_centered\u001b[49m\u001b[43m=\u001b[49m\u001b[43massume_centered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstore_precision\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator.covariance_, estimator.shrinkage_\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/image-anomaly-detection/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/image-anomaly-detection/.venv/lib/python3.12/site-packages/sklearn/covariance/_shrunk_covariance.py:607\u001b[39m, in \u001b[36mLedoitWolf.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    606\u001b[39m     \u001b[38;5;28mself\u001b[39m.location_ = X.mean(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m covariance, shrinkage = \u001b[43m_ledoit_wolf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlocation_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_centered\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock_size\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[38;5;28mself\u001b[39m.shrinkage_ = shrinkage\n\u001b[32m    611\u001b[39m \u001b[38;5;28mself\u001b[39m._set_covariance(covariance)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/image-anomaly-detection/.venv/lib/python3.12/site-packages/sklearn/covariance/_shrunk_covariance.py:35\u001b[39m, in \u001b[36m_ledoit_wolf\u001b[39m\u001b[34m(X, assume_centered, block_size)\u001b[39m\n\u001b[32m     32\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# get Ledoit-Wolf shrinkage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m shrinkage = \u001b[43mledoit_wolf_shrinkage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_centered\u001b[49m\u001b[43m=\u001b[49m\u001b[43massume_centered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m emp_cov = empirical_covariance(X, assume_centered=assume_centered)\n\u001b[32m     39\u001b[39m mu = np.sum(np.trace(emp_cov)) / n_features\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/image-anomaly-detection/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:191\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m global_skip_validation = get_config()[\u001b[33m\"\u001b[39m\u001b[33mskip_parameter_validation\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m func_sig = signature(func)\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/work/image-anomaly-detection/.venv/lib/python3.12/site-packages/sklearn/covariance/_shrunk_covariance.py:374\u001b[39m, in \u001b[36mledoit_wolf_shrinkage\u001b[39m\u001b[34m(X, assume_centered, block_size)\u001b[39m\n\u001b[32m    372\u001b[39m     cols = \u001b[38;5;28mslice\u001b[39m(block_size * j, block_size * (j + \u001b[32m1\u001b[39m))\n\u001b[32m    373\u001b[39m     beta_ += np.sum(np.dot(X2.T[rows], X2[:, cols]))\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m     delta_ += np.sum(\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m ** \u001b[32m2\u001b[39m)\n\u001b[32m    375\u001b[39m rows = \u001b[38;5;28mslice\u001b[39m(block_size * i, block_size * (i + \u001b[32m1\u001b[39m))\n\u001b[32m    376\u001b[39m beta_ += np.sum(np.dot(X2.T[rows], X2[:, block_size * n_splits :]))\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "#with warnings.catch_warnings():\n",
        "#    # UserWarningを非表示\n",
        "#    warnings.filterwarnings('ignore', category=UserWarning) \n",
        "all_rows = []\n",
        "for cat in eval_categories:\n",
        "    print(f'[RUN] category={cat}, backbone={backbone}')\n",
        "    out = evaluate_category(cat, backbone)\n",
        "    print('completed.')\n",
        "    ts = datetime.now(timezone.utc).isoformat()\n",
        "    # 保存\n",
        "    out_dir = RESULTS / 'eval' / cat\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for method, metrics in out['results'].items():\n",
        "        with (out_dir / f'metrics_{method}.json').open('w', encoding='utf-8') as f:\n",
        "            json.dump({\n",
        "                'category': cat, 'method': method, 'backbone': backbone,\n",
        "                'timestamp': ts, 'metrics': metrics\n",
        "            }, f, indent=2, ensure_ascii=False)\n",
        "        row = {'category': cat, 'method': method, 'backbone': backbone}\n",
        "        row.update(metrics)\n",
        "        all_rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(all_rows)\n",
        "df_pivot = df.pivot(index='category', columns='method', values='auroc')\n",
        "print('[Summary] AUROC by method/category')\n",
        "display(df_pivot)\n",
        "print('[All metrics]')\n",
        "display(df)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "image-anomaly-detection",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
