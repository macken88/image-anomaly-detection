{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b521ae5d",
   "metadata": {},
   "source": [
    "# experiment.ipynb — MVTec 画像異常検知 実験ノート\n",
    "\n",
    "複数バックボーン × 複数カテゴリの組み合わせで Mahalanobis 距離法と PaDiM を評価し、OOF スコアから求めた z 閾値で最終モデルを検証します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4182e3e",
   "metadata": {},
   "source": [
    "## 1. ライブラリ読み込み\n",
    "すべての依存ライブラリを最初のセルに集約します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.covariance import ledoit_wolf\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.models import get_model, get_model_weights\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "# 独自モジュール\n",
    "from anomaly_detectors import (\n",
    "    fit_mahalanobis,\n",
    "    all_mahalanobis_scores,\n",
    "    fit_padim,\n",
    "    all_padim_scores,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4030b3c",
   "metadata": {},
   "source": [
    "## 2. 共通設定と実験用ディレクトリ\n",
    "乱数シードやデバイス、結果保存先などをまとめて初期化します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共通設定をまとめて定義し、再現性と出力先を揃える\n",
    "RANDOM_SEED = 0\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PIN_MEMORY = device.type == \"cuda\"\n",
    "NUM_WORKERS = min(4, os.cpu_count() or 1)\n",
    "\n",
    "MVTEC_ROOT = Path(os.environ.get(\"MVTEC_ROOT\", \"datasets/MVTecAD\"))\n",
    "MVTEC_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "RESULT_ROOT = Path(f\"results/dev/{RUN_TIMESTAMP}\")\n",
    "RESULT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEFAULT_BATCH_SIZE = 16\n",
    "DEFAULT_K_SPLITS = 5\n",
    "threshold_percentile = 99\n",
    "padim_channel_subsample = 100\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "print(f\"[INFO] device={device}, pin_memory={PIN_MEMORY}, num_workers={NUM_WORKERS}\")\n",
    "print(f\"[INFO] result root: {RESULT_ROOT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6eed319",
   "metadata": {},
   "source": [
    "### 2.2 データ取得（anomalib）\n",
    "MVTec のデータが存在しない場合は anomalib を用いてダウンロードします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90816e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MVTec データの有無を確認し、必要なら anomalib でダウンロード\n",
    "try:\n",
    "    from anomalib.data import MVTecAD\n",
    "except ImportError:\n",
    "    print(\"[WARN] anomalib が見つかりません。事前にインストールしてください。\")\n",
    "else:\n",
    "    try:\n",
    "        datamodule = MVTecAD(root=str(MVTEC_ROOT))\n",
    "        datamodule.prepare_data()\n",
    "        print(f\"[INFO] downloaded/prepared MVTec data under {MVTEC_ROOT}\")\n",
    "    except Exception as exc:\n",
    "        print(f\"[WARN] anomalib のデータ準備でエラー: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f7c6ea",
   "metadata": {},
   "source": [
    "## 3. バックボーン読み込みヘルパー\n",
    "`torchvision` のモデル名からバックボーンを構築します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12c4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_backbone_from_name(name: str) -> nn.Module:\n",
    "    \"\"\"バックボーン名から学習済みモデルを読み込み、評価モードで返す。\"\"\"\n",
    "    try:\n",
    "        weights = None\n",
    "        try:\n",
    "            weights = get_model_weights(name).DEFAULT\n",
    "        except (AttributeError, ValueError, RuntimeError):\n",
    "            pass  # ウェイトが無いモデルはランダム初期化\n",
    "        model = get_model(name, weights=weights)\n",
    "    except Exception as exc:\n",
    "        raise ValueError(f\"未対応のバックボーンです: {name}\") from exc\n",
    "    return model.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a360e96d",
   "metadata": {},
   "source": [
    "## 4. データセット関連ユーティリティ\n",
    "MVTec データの読み込み、KFold 分割、DataLoader の組み立てを関数化します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205824da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transform(image_size: int) -> transforms.Compose:\n",
    "    \"\"\"画像サイズに合わせた前処理パイプラインを作成する。\"\"\"\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "\n",
    "\n",
    "class ImagePathDataset(Dataset):\n",
    "    \"\"\"ファイルパスから画像テンソルとラベルを返す簡易データセット。\"\"\"\n",
    "\n",
    "    def __init__(self, paths: List[Path], labels: List[Any], transform=None):\n",
    "        self.paths = [Path(p) for p in paths]\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        with Image.open(img_path) as img:\n",
    "            img = img.convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def _existing_category_root(category: str) -> Path:\n",
    "    \"\"\"カテゴリのルートディレクトリを既知のパターンから探す。\"\"\"\n",
    "    candidates = [\n",
    "        MVTEC_ROOT / category,\n",
    "        Path(\"datasets/MVTecAD\") / category,\n",
    "        Path(\"MVtec_dataset\") / category,\n",
    "    ]\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"MVTec category not found: {category}\")\n",
    "\n",
    "\n",
    "def _list_images(directory: Path) -> List[Path]:\n",
    "    \"\"\"指定ディレクトリ以下の画像パスを拡張子フィルタ付きで列挙する。\"\"\"\n",
    "    exts = {\".png\", \".jpg\", \".jpeg\"}\n",
    "    if not directory.exists():\n",
    "        return []\n",
    "    return sorted([p for p in directory.rglob('*') if p.suffix.lower() in exts])\n",
    "\n",
    "\n",
    "def build_full_train_loader(\n",
    "    category: str,\n",
    "    *,\n",
    "    batch_size: int,\n",
    "    transform,\n",
    "    pin_memory: bool,\n",
    "    num_workers: int,\n",
    ") -> DataLoader:\n",
    "    \"\"\"カテゴリ全体の train/good 画像を読み込み用 DataLoader にする。\"\"\"\n",
    "    root = _existing_category_root(category)\n",
    "    train_paths = _list_images(root / 'train' / 'good')\n",
    "    assert train_paths, f\"No train/good images found for {category}\"\n",
    "    dataset = ImagePathDataset(train_paths, [0] * len(train_paths), transform=transform)\n",
    "    return DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_cv_and_test_loaders(\n",
    "    category: str,\n",
    "    *,\n",
    "    k_splits: int,\n",
    "    batch_size: int,\n",
    "    transform,\n",
    "    pin_memory: bool,\n",
    "    num_workers: int,\n",
    ") -> Tuple[List[Dict[str, Any]], DataLoader]:\n",
    "    \"\"\"train/good を KFold で分割し、CV 用と dev test 用 DataLoader を返す。\"\"\"\n",
    "    root = _existing_category_root(category)\n",
    "    train_good = _list_images(root / 'train' / 'good')\n",
    "    assert train_good, f\"No train/good images found for {category}\"\n",
    "\n",
    "    base_ds = ImagePathDataset(train_good, [0] * len(train_good), transform=transform)\n",
    "    kf = KFold(n_splits=k_splits, shuffle=True, random_state=0)\n",
    "\n",
    "    cv_folds: List[Dict[str, Any]] = []\n",
    "    for fold_id, (tr_idx, va_idx) in enumerate(kf.split(range(len(train_good)))):\n",
    "        train_subset = Subset(base_ds, tr_idx)\n",
    "        val_subset = Subset(base_ds, va_idx)\n",
    "        train_loader = DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "        cv_folds.append(\n",
    "            {\n",
    "                \"fold\": fold_id,\n",
    "                \"train_loader\": train_loader,\n",
    "                \"val_loader\": val_loader,\n",
    "                \"n_train\": len(tr_idx),\n",
    "                \"n_val\": len(va_idx),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    test_dir = root / 'test'\n",
    "    test_paths: List[Path] = []\n",
    "    test_labels: List[str] = []\n",
    "    if test_dir.exists():\n",
    "        for sub in sorted([d for d in test_dir.iterdir() if d.is_dir()], key=lambda p: p.name):\n",
    "            paths = _list_images(sub)\n",
    "            if paths:\n",
    "                test_paths.extend(paths)\n",
    "                test_labels.extend([sub.name] * len(paths))\n",
    "    assert test_paths, f\"No test images found for {category}\"\n",
    "\n",
    "    test_dataset = ImagePathDataset(test_paths, test_labels, transform=transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return cv_folds, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2bf7dc",
   "metadata": {},
   "source": [
    "## 5. 可視化ユーティリティ\n",
    "fold ごとのスコア分布をヒストグラムで保存します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(\n",
    "    df: pd.DataFrame,\n",
    "    ifold: int,\n",
    "    save_dir: Path,\n",
    "    method_name: str,\n",
    "    show_image: bool = False,\n",
    ") -> None:\n",
    "    \"\"\"スコアのヒストグラムを描画・保存し、必要に応じて表示する。\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"log_score\"] = np.log1p(df[\"score\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    n_bins = 30\n",
    "    bin_edges = np.linspace(df[\"log_score\"].min(), df[\"log_score\"].max(), n_bins + 1)\n",
    "\n",
    "    preferred = [\"train\", \"val\", \"good\"]\n",
    "    labels_all = list(dict.fromkeys(df[\"label\"].tolist()))\n",
    "    ordered_labels = preferred + [lb for lb in labels_all if lb not in preferred]\n",
    "\n",
    "    for label in ordered_labels:\n",
    "        values = df.loc[df[\"label\"] == label, \"log_score\"].to_numpy()\n",
    "        if len(values) == 0:\n",
    "            continue\n",
    "        ax.hist(\n",
    "            values,\n",
    "            bins=bin_edges,\n",
    "            alpha=0.6,\n",
    "            label=label,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.3,\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"{method_name} / Fold {ifold} — log(score)\")\n",
    "    ax.set_xlabel(\"log_score\")\n",
    "    ax.set_ylabel(\"count\")\n",
    "    ax.legend(fontsize=8, ncol=2)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_png = save_dir / f\"fold_{ifold}_hist.png\"\n",
    "    fig.savefig(out_png, dpi=150)\n",
    "    if show_image:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406058f",
   "metadata": {},
   "source": [
    "## 6. パイプライン本体\n",
    "Mahalanobis / PaDiM の学習・評価を一貫処理する関数を定義します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48953001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_cpu(obj):\n",
    "    \"\"\"テンソルやテンソルのリストを CPU 側に集約する補助関数。\"\"\"\n",
    "    if isinstance(obj, torch.Tensor):\n",
    "        return obj.detach().cpu()\n",
    "    if isinstance(obj, list):\n",
    "        return [_to_cpu(o) for o in obj]\n",
    "    if isinstance(obj, tuple):\n",
    "        return tuple(_to_cpu(o) for o in obj)\n",
    "    return obj\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MethodSpec:\n",
    "    key: str\n",
    "    display_name: str\n",
    "    fit_fn: Callable[..., Any]\n",
    "    score_fn: Callable[..., Any]\n",
    "    fit_kwargs: Dict[str, Any]\n",
    "    score_kwargs_common: Dict[str, Any]\n",
    "    score_kwargs_test: Dict[str, Any]\n",
    "    output_dir: Path\n",
    "    collects_heatmaps: bool = False\n",
    "\n",
    "\n",
    "def save_padim_heatmaps(heatmaps, out_dir: Path, cmap: str = \"hot\") -> int:\n",
    "    \"\"\"PaDiM のヒートマップを保存し、保存枚数を返す。\"\"\"\n",
    "    if heatmaps is None:\n",
    "        return 0\n",
    "\n",
    "    arrays = []\n",
    "    for hm in heatmaps:\n",
    "        if isinstance(hm, torch.Tensor):\n",
    "            arrays.append(hm.detach().cpu().numpy())\n",
    "        else:\n",
    "            arrays.append(np.asarray(hm))\n",
    "\n",
    "    if not arrays:\n",
    "        return 0\n",
    "\n",
    "    vmin = float(min(hm.min() for hm in arrays))\n",
    "    vmax = float(max(hm.max() for hm in arrays))\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for idx, hm in enumerate(arrays):\n",
    "        out_png = out_dir / f\"heatmap_{idx:05d}.png\"\n",
    "        plt.imsave(out_png, hm, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    return len(arrays)\n",
    "\n",
    "\n",
    "def run_pipeline(\n",
    "    spec: MethodSpec,\n",
    "    model: nn.Module,\n",
    "    *,\n",
    "    category: str,\n",
    "    transform,\n",
    "    device: torch.device,\n",
    "    k_splits: int,\n",
    "    batch_size: int,\n",
    "    oof_percentile: float,\n",
    "    pin_memory: bool,\n",
    "    num_workers: int,\n",
    "    plot_histogram_fn: Callable[..., None] = plot_histogram,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"1 メソッド分の CV→OOF→最終モデル学習を実行する。\"\"\"\n",
    "    spec.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cv_folds, dev_test_loader = build_cv_and_test_loaders(\n",
    "        category=category,\n",
    "        k_splits=k_splits,\n",
    "        batch_size=batch_size,\n",
    "        transform=transform,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    fold_artifacts: Dict[int, Dict[str, Any]] = {}\n",
    "    metric_rows: List[Dict[str, float]] = []\n",
    "    oof_normal_scores_per_fold: List[np.ndarray] = []\n",
    "\n",
    "    test_labels = np.array([0 if lbl == \"good\" else 1 for lbl in dev_test_loader.dataset.labels])\n",
    "\n",
    "    for ifold, fold in enumerate(cv_folds):\n",
    "        fit_kwargs = dict(spec.fit_kwargs)\n",
    "        fit_kwargs.setdefault(\"device\", device)\n",
    "        model_state = spec.fit_fn(fold[\"train_loader\"], model, **fit_kwargs)\n",
    "\n",
    "        scores_train = _to_cpu(spec.score_fn(model_state, fold[\"train_loader\"], **spec.score_kwargs_common))\n",
    "        scores_val = _to_cpu(spec.score_fn(model_state, fold[\"val_loader\"], **spec.score_kwargs_common))\n",
    "        oof_normal_scores_per_fold.append(scores_val.numpy())\n",
    "\n",
    "        test_out = spec.score_fn(model_state, dev_test_loader, **spec.score_kwargs_test)\n",
    "        if spec.collects_heatmaps:\n",
    "            scores_test, heatmaps_test = test_out\n",
    "        else:\n",
    "            scores_test, heatmaps_test = test_out, None\n",
    "        scores_test = _to_cpu(scores_test)\n",
    "        if heatmaps_test is not None:\n",
    "            heatmaps_test = _to_cpu(heatmaps_test)\n",
    "\n",
    "        fold_artifacts[ifold] = {\n",
    "            \"model_state\": model_state,\n",
    "            \"scores_train\": scores_train,\n",
    "            \"scores_val\": scores_val,\n",
    "            \"scores_test\": scores_test,\n",
    "            \"heatmaps_test\": heatmaps_test,\n",
    "        }\n",
    "\n",
    "        scores_val_np = scores_val.numpy()\n",
    "        scores_test_np = scores_test.numpy()\n",
    "        fold_threshold = float(np.quantile(scores_val_np, oof_percentile))\n",
    "        preds_test = (scores_test_np >= fold_threshold).astype(int)\n",
    "        auc = roc_auc_score(test_labels, scores_test_np)\n",
    "        f1 = f1_score(test_labels, preds_test)\n",
    "        metric_rows.append(\n",
    "            {\n",
    "                \"fold\": ifold,\n",
    "                \"threshold_raw@fold\": fold_threshold,\n",
    "                \"auc\": auc,\n",
    "                \"f1\": f1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df_hist = pd.DataFrame(\n",
    "            {\n",
    "                \"score\": np.r_[\n",
    "                    scores_train.numpy(),\n",
    "                    scores_val.numpy(),\n",
    "                    scores_test.numpy(),\n",
    "                ],\n",
    "                \"label\": (\n",
    "                    [\"train\"] * len(scores_train)\n",
    "                    + [\"val\"] * len(scores_val)\n",
    "                    + dev_test_loader.dataset.labels\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        plot_histogram_fn(df_hist, ifold, spec.output_dir, spec.display_name)\n",
    "\n",
    "        print(\n",
    "            f\"[{spec.display_name} Fold {ifold}] Raw-th@{int(oof_percentile*100)}%: {fold_threshold:.4f}, \"\n",
    "            f\"AUC: {auc:.4f}, F1: {f1:.4f}\"\n",
    "        )\n",
    "\n",
    "    metrics_df = pd.DataFrame(metric_rows)\n",
    "    metrics_df.to_csv(spec.output_dir / \"summary.csv\", index=False)\n",
    "    auc_mean = float(metrics_df[\"auc\"].mean())\n",
    "    auc_std = float(metrics_df[\"auc\"].std())\n",
    "    f1_mean = float(metrics_df[\"f1\"].mean())\n",
    "    f1_std = float(metrics_df[\"f1\"].std())\n",
    "    print(\n",
    "        f\"[{spec.display_name}] CV AUC: {auc_mean:.4f} ± {auc_std:.4f}, \"\n",
    "        f\"F1: {f1_mean:.4f} ± {f1_std:.4f}\"\n",
    "    )\n",
    "\n",
    "    assert oof_normal_scores_per_fold, \"OOF scores are empty.\"\n",
    "    oof_normal_scores = np.concatenate(oof_normal_scores_per_fold)\n",
    "    threshold_oof_raw = float(np.quantile(oof_normal_scores, oof_percentile))\n",
    "    mu_oof = float(np.mean(oof_normal_scores))\n",
    "    sigma_oof = float(np.std(oof_normal_scores, ddof=1))\n",
    "    denom_oof = sigma_oof if sigma_oof > 0 else 1.0\n",
    "    z_threshold = float(np.quantile((oof_normal_scores - mu_oof) / denom_oof, oof_percentile))\n",
    "\n",
    "    print({\n",
    "        \"OOF_N\": len(oof_normal_scores),\n",
    "        \"threshold_oof_raw\": threshold_oof_raw,\n",
    "        \"mu_oof\": mu_oof,\n",
    "        \"sigma_oof\": sigma_oof,\n",
    "        \"z_threshold\": z_threshold,\n",
    "    })\n",
    "\n",
    "    final_train_loader = build_full_train_loader(\n",
    "        category,\n",
    "        batch_size=batch_size,\n",
    "        transform=transform,\n",
    "        pin_memory=pin_memory,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    final_fit_kwargs = dict(spec.fit_kwargs)\n",
    "    final_fit_kwargs.setdefault(\"device\", device)\n",
    "    final_model_state = spec.fit_fn(final_train_loader, model, **final_fit_kwargs)\n",
    "\n",
    "    final_train_scores = _to_cpu(\n",
    "        spec.score_fn(final_model_state, final_train_loader, **spec.score_kwargs_common)\n",
    "    ).numpy()\n",
    "    mu_final = float(np.mean(final_train_scores))\n",
    "    sigma_final = float(np.std(final_train_scores, ddof=1))\n",
    "    denom_final = sigma_final if sigma_final > 0 else 1.0\n",
    "\n",
    "    final_out = spec.score_fn(final_model_state, dev_test_loader, **spec.score_kwargs_test)\n",
    "    if spec.collects_heatmaps:\n",
    "        final_test_scores, final_heatmaps = final_out\n",
    "    else:\n",
    "        final_test_scores, final_heatmaps = final_out, None\n",
    "    final_test_scores = _to_cpu(final_test_scores).numpy()\n",
    "    final_preds = ((final_test_scores - mu_final) / denom_final > z_threshold).astype(int)\n",
    "    final_auc = float(roc_auc_score(test_labels, final_test_scores))\n",
    "    final_f1 = float(f1_score(test_labels, final_preds))\n",
    "\n",
    "    print(\n",
    "        f\"[{spec.display_name} Final] AUC: {final_auc:.4f}, F1(z@{int(oof_percentile*100)}%): {final_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"fold_artifacts\": fold_artifacts,\n",
    "        \"metrics\": metrics_df,\n",
    "        \"auc_mean\": auc_mean,\n",
    "        \"auc_std\": auc_std,\n",
    "        \"f1_mean\": f1_mean,\n",
    "        \"f1_std\": f1_std,\n",
    "        \"oof\": {\n",
    "            \"oof_normal_scores_per_fold\": oof_normal_scores_per_fold,\n",
    "            \"threshold_oof_raw\": threshold_oof_raw,\n",
    "            \"mu_oof\": mu_oof,\n",
    "            \"sigma_oof\": sigma_oof,\n",
    "            \"z_threshold\": z_threshold,\n",
    "        },\n",
    "        \"final\": {\n",
    "            \"model_state\": final_model_state,\n",
    "            \"mu_final\": mu_final,\n",
    "            \"sigma_final\": sigma_final,\n",
    "            \"test_scores\": final_test_scores,\n",
    "            \"preds\": final_preds,\n",
    "            \"auc\": final_auc,\n",
    "            \"f1\": final_f1,\n",
    "            \"heatmaps_test\": final_heatmaps,\n",
    "        },\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f6a34",
   "metadata": {},
   "source": [
    "## 7. スイープ設定と実行\n",
    "バックボーン・カテゴリの組み合わせでパイプラインをまとめて実行します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b45dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE_CFG = {\n",
    "    \"resnet18\": {\n",
    "        \"image_size\": 256,\n",
    "        \"md_layer\": \"flatten\",\n",
    "        \"padim_layers\": [\"layer1.1.relu_1\", \"layer2.1.relu_1\", \"layer3.1.relu_1\"],\n",
    "        \"d\": padim_channel_subsample,\n",
    "    },\n",
    "    \"efficientnet_b0\": {\n",
    "        \"image_size\": 256,\n",
    "        \"md_layer\": \"flatten\",\n",
    "        \"padim_layers\": [\n",
    "            \"features.6.3.add\",\n",
    "            \"features.7.0.block.0\",\n",
    "            \"features.7.0.block.1\",\n",
    "            \"features.7.0.block.2\",\n",
    "            \"features.7.0.block.3\",\n",
    "        ],\n",
    "        \"d\": padim_channel_subsample,\n",
    "    },\n",
    "}\n",
    "\n",
    "# 実行対象を必要に応じて編集する\n",
    "TARGET_BACKBONES = [\"resnet18\"]\n",
    "TARGET_CATEGORIES = [\"carpet\"]\n",
    "\n",
    "sweep_records: List[Dict[str, Any]] = []\n",
    "\n",
    "for backbone_name in TARGET_BACKBONES:\n",
    "    if backbone_name not in BACKBONE_CFG:\n",
    "        print(f\"[WARN] 未対応バックボーンをスキップ: {backbone_name}\")\n",
    "        continue\n",
    "\n",
    "    cfg = BACKBONE_CFG[backbone_name]\n",
    "    transform = make_transform(cfg[\"image_size\"])\n",
    "    model = load_backbone_from_name(backbone_name)\n",
    "\n",
    "    for category in TARGET_CATEGORIES:\n",
    "        combo_root = RESULT_ROOT / backbone_name / category\n",
    "        md_dir = combo_root / \"MD\"\n",
    "        padim_dir = combo_root / \"PaDiM\"\n",
    "        md_dir.mkdir(parents=True, exist_ok=True)\n",
    "        padim_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        method_specs = [\n",
    "            MethodSpec(\n",
    "                key=\"mahalanobis\",\n",
    "                display_name=\"Mahalanobis\",\n",
    "                fit_fn=fit_mahalanobis,\n",
    "                score_fn=all_mahalanobis_scores,\n",
    "                fit_kwargs={\"feature_node\": cfg[\"md_layer\"]},\n",
    "                score_kwargs_common={},\n",
    "                score_kwargs_test={},\n",
    "                output_dir=md_dir,\n",
    "            ),\n",
    "            MethodSpec(\n",
    "                key=\"padim\",\n",
    "                display_name=\"PaDiM\",\n",
    "                fit_fn=fit_padim,\n",
    "                score_fn=all_padim_scores,\n",
    "                fit_kwargs={\"layers\": cfg[\"padim_layers\"], \"d\": cfg[\"d\"]},\n",
    "                score_kwargs_common={},\n",
    "                score_kwargs_test={\"return_maps\": True},\n",
    "                output_dir=padim_dir,\n",
    "                collects_heatmaps=True,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        for spec in method_specs:\n",
    "            print(f\"[RUN] backbone={backbone_name}, category={category}, method={spec.display_name}\")\n",
    "            result = run_pipeline(\n",
    "                spec=spec,\n",
    "                model=model,\n",
    "                category=category,\n",
    "                transform=transform,\n",
    "                device=device,\n",
    "                k_splits=DEFAULT_K_SPLITS,\n",
    "                batch_size=DEFAULT_BATCH_SIZE,\n",
    "                oof_percentile=threshold_percentile / 100.0,\n",
    "                pin_memory=PIN_MEMORY,\n",
    "                num_workers=NUM_WORKERS,\n",
    "            )\n",
    "\n",
    "            sweep_records.append(\n",
    "                {\n",
    "                    \"backbone\": backbone_name,\n",
    "                    \"category\": category,\n",
    "                    \"method\": spec.display_name,\n",
    "                    \"cv_auc_mean\": result[\"auc_mean\"],\n",
    "                    \"cv_auc_std\": result[\"auc_std\"],\n",
    "                    \"cv_f1_mean\": result[\"f1_mean\"],\n",
    "                    \"cv_f1_std\": result[\"f1_std\"],\n",
    "                    \"final_auc\": result[\"final\"][\"auc\"],\n",
    "                    \"final_f1\": result[\"final\"][\"f1\"],\n",
    "                    \"z_threshold\": result[\"oof\"][\"z_threshold\"],\n",
    "                    \"mu_final\": result[\"final\"][\"mu_final\"],\n",
    "                    \"sigma_final\": result[\"final\"][\"sigma_final\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if spec.key == \"padim\":\n",
    "                saved = save_padim_heatmaps(\n",
    "                    result[\"final\"][\"heatmaps_test\"],\n",
    "                    spec.output_dir / \"final_heatmaps\",\n",
    "                )\n",
    "                if saved:\n",
    "                    print(f\"[INFO] Saved {saved} heatmaps -> {spec.output_dir / 'final_heatmaps'}\")\n",
    "\n",
    "sweep_df = pd.DataFrame(sweep_records)\n",
    "sweep_csv = RESULT_ROOT / \"sweep_summary.csv\"\n",
    "sweep_df.to_csv(sweep_csv, index=False)\n",
    "print(f\"[INFO] Saved sweep summary CSV: {sweep_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee917f54",
   "metadata": {},
   "source": [
    "## 8. スイープ結果プレビュー\n",
    "集計結果を DataFrame として確認します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not sweep_records:\n",
    "    print(\"[WARN] まだスイープが実行されていません。前セルを実行してください。\")\n",
    "else:\n",
    "    display(sweep_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-anomaly-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
