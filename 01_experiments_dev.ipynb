{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_experiments_dev — MVTec 画像異常検知（開発\n",
        "\n",
        "本ノートは dev カテゴリのみで設計を確定し、固定パイプライン設定を `assets/fixed_pipeline.json` に出力するためのテンプレートです。\n",
        "- データ取得は anomalib を用いる（AGENTS.md 準拠）\n",
        "- 手法は Mahalanobis / PaDiM を比較\n",
        "- 閾値は dev の test で画像レベル FPR=1% を目標に決定\n",
        "\n",
        "実行順序：Header → Data → Methods → Results → Save JSON/Artifacts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 環境・依存の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe51156d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')  # すべての警告を非表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5768fe73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 参考: anomaly_detection.ipynb からの初期インポートを整理\n",
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from sklearn.covariance import ledoit_wolf\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# anomaly_core からコア関数をインポート\n",
        "from anomaly_core import (\n",
        "    fit_mahalanobis, all_mahalanobis_scores,\n",
        "    fit_padim, padim_heatmap, all_padim_scores,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65088a9b",
      "metadata": {},
      "source": [
        "## データ取得（anomalib 経由）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01228e6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# AGENTS.md: 既存の MVTEC_ROOT または datasets/MVTecAD を使用。\n",
        "# 未検出の場合は anomalib によりダウンロード。\n",
        "MVTEC_ROOT = Path(os.environ.get(\"MVTEC_ROOT\", \"datasets/MVTecAD\"))\n",
        "MVTEC_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# anomalib のAPIはバージョンで異なる可能性があるため、例示的に記述。\n",
        "# 実環境の anomalib バージョンに合わせて import と引数を調整してください。\n",
        "try:\n",
        "    from anomalib.data import MVTecAD\n",
        "    datamodule = MVTecAD(root=str(MVTEC_ROOT))\n",
        "    datamodule.prepare_data()  # download if needed\n",
        "    datamodule.setup()\n",
        "except Exception as e:\n",
        "    print(\"[WARN] anomalib のデータ取得セットアップで問題が発生しました。バージョンや引数を確認してください:\\n\", e)\n",
        "\n",
        "assert MVTEC_ROOT.exists(), \"MVTec root not found after anomalib setup.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe101f49",
      "metadata": {},
      "source": [
        "## 実験設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d49852b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# dev カテゴリと seed を定義\n",
        "dev_category = \"carpet\"  # 例: AGENTS.md 推奨例\n",
        "seeds = [0, 1, 2]\n",
        "image_size = 256\n",
        "\n",
        "# 比較する手法（最小構成）\n",
        "#use_mahalanobis = True\n",
        "#use_padim = True\n",
        "\n",
        "# PaDiM や Mahalanobis で用いる backbone/layers 等は仮パラメータ（要調整）\n",
        "backbone = \"efficientnet_b0\"\n",
        "padim_layers = ['features.7.0.block.0', 'features.7.0.block.1', 'features.7.0.block.2', 'features.7.0.block.3']\n",
        "# efficientnet:['features.6.3.add', 'features.7.0.block.0', 'features.7.0.block.1', 'features.7.0.block.2', 'features.7.0.block.3', 'features.8']\n",
        "# resnet:[\"layer1\", \"layer2\", \"layer3\"]\n",
        "padim_channel_subsample = 100\n",
        "#cov_estimator = \"ledoit_wolf\"  # Mahalanobis 用\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74d370c",
      "metadata": {},
      "source": [
        "## 学習と推論\n",
        "- 各手法で dev の train を用いたクロスバリデーションを実施し、訓練内/訓練外スコアのヒストグラムを確認する。\n",
        "- dev の test で閾値と評価指標（AUROC, F1 など）の関係を可視化する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da5dc09a",
      "metadata": {},
      "source": [
        "### データローダー準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68fb8377",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data loading for cross-validation (dev train) and dev test\n",
        "# - Builds KFold train/val DataLoaders using only train/good images.\n",
        "# - Prepares dev test DataLoader with labels (good=0, defect=1).\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Any\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import os\n",
        "\n",
        "# Transforms (ImageNet mean/std)\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "])\n",
        "\n",
        "class ImagePathDataset(Dataset):\n",
        "    \"\"\"Minimal dataset returning (image_tensor, label).\n",
        "\n",
        "    Paths: list of filesystem paths; Labels: list[Any] of same length.\n",
        "    \"\"\"\n",
        "    def __init__(self, paths: List[Path], labels: List[Any], transform=None):\n",
        "        self.paths = [Path(p) for p in paths]\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        y = self.labels[idx]\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, y\n",
        "\n",
        "def _existing_category_root(category: str) -> Path:\n",
        "    \"\"\"Find an existing MVTec category root among common layouts.\n",
        "    Prefers MVTEC_ROOT, then 'datasets/MVTecAD', then 'MVtec_dataset'.\n",
        "    \"\"\"\n",
        "    candidates = [\n",
        "        MVTEC_ROOT / category,\n",
        "        Path(\"datasets/MVTecAD\") / category,\n",
        "        Path(\"MVtec_dataset\") / category,\n",
        "    ]\n",
        "    for c in candidates:\n",
        "        if c.exists():\n",
        "            return c\n",
        "    raise FileNotFoundError(f\"MVTec category not found: {category}\")\n",
        "\n",
        "def _list_images(d: Path) -> List[Path]:\n",
        "    exts = {\".png\", \".jpg\", \".jpeg\"}\n",
        "    if not d.exists():\n",
        "        return []\n",
        "    return sorted([p for p in d.rglob('*') if p.suffix.lower() in exts])\n",
        "\n",
        "def build_cv_and_test_loaders(category: str, k_splits: int = 5, batch_size: int = 32) -> Tuple[list, DataLoader]:\n",
        "    \"\"\"Return (cv_folds, test_loader).\n",
        "\n",
        "    cv_folds: list of dicts with 'train_loader' and 'val_loader'.\n",
        "    test_loader: dev test DataLoader with labels equal to defect types\n",
        "                 (the directory names directly under 'test', e.g., 'good',\n",
        "                 'scratch', 'hole', ...).\n",
        "    \"\"\"\n",
        "    root = _existing_category_root(category)\n",
        "    train_good = _list_images(root / 'train' / 'good')\n",
        "    assert len(train_good) > 0, f\"No train/good images found for {category}\"\n",
        "\n",
        "    # Prepare KFold over indices (all labels are 0 in train).\n",
        "    kf = KFold(n_splits=k_splits, shuffle=True, random_state=seeds[0] if seeds else 0)\n",
        "    base_ds = ImagePathDataset(train_good, [0] * len(train_good), transform=_transform)\n",
        "\n",
        "    num_workers = min(4, os.cpu_count() or 1)\n",
        "    cv_folds = []\n",
        "    for fold_id, (tr_idx, va_idx) in enumerate(kf.split(range(len(train_good)))):\n",
        "        tr_ds = Subset(base_ds, tr_idx)\n",
        "        va_ds = Subset(base_ds, va_idx)\n",
        "        tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "        va_loader = DataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "        cv_folds.append({\n",
        "            'fold': fold_id,\n",
        "            'train_loader': tr_loader,\n",
        "            'val_loader': va_loader,\n",
        "            'n_train': len(tr_idx),\n",
        "            'n_val': len(va_idx),\n",
        "        })\n",
        "\n",
        "    # Build dev test loader with labels as defect types (dir names under 'test').\n",
        "    test_dir = root / 'test'\n",
        "    test_paths: List[Path] = []\n",
        "    test_labels: List[str] = []\n",
        "    if test_dir.exists():\n",
        "        # Iterate over subdirectories directly under 'test' (including 'good').\n",
        "        for sub in sorted([d for d in test_dir.iterdir() if d.is_dir()], key=lambda p: p.name):\n",
        "            label = sub.name  # defect type (or 'good')\n",
        "            paths = _list_images(sub)\n",
        "            if paths:\n",
        "                test_paths.extend(paths)\n",
        "                test_labels.extend([label] * len(paths))\n",
        "    assert len(test_paths) > 0, f\"No test images found for {category}\"\n",
        "\n",
        "    test_ds = ImagePathDataset(test_paths, test_labels, transform=_transform)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return cv_folds, test_loader\n",
        "\n",
        "# Build loaders for the chosen dev category\n",
        "cv_folds, dev_test_loader = build_cv_and_test_loaders(dev_category, k_splits=5, batch_size=32)\n",
        "print(f\"[INFO] Category: {dev_category}\")\n",
        "print(f\"[INFO] CV folds: {len(cv_folds)}\")\n",
        "for f in cv_folds:\n",
        "    print(f\"  - fold {f['fold']}: n_train={f['n_train']}, n_val={f['n_val']}\")\n",
        "from collections import Counter\n",
        "cnt = Counter(dev_test_loader.dataset.labels)\n",
        "print(f\"[INFO] Dev test size: {len(dev_test_loader.dataset)}\")\n",
        "print(f\"[INFO] Test label distribution: {dict(cnt)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbb268b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031692ae",
      "metadata": {},
      "source": [
        "### マハラノビス距離ベースでの異常検知実験"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b189f4dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CV 各フォールドでマハラノビス距離ベースモデルを学習・評価\n",
        "all_results_MD = {}\n",
        "for ifold, fold in enumerate(cv_folds):\n",
        "    model_state = fit_mahalanobis(fold[\"train_loader\"], backbone, device=device)\n",
        "    scores_train = all_mahalanobis_scores(model_state, fold[\"train_loader\"])\n",
        "    scores_val = all_mahalanobis_scores(model_state, fold[\"val_loader\"])\n",
        "    scores_test = all_mahalanobis_scores(model_state, dev_test_loader)\n",
        "\n",
        "    all_results_MD[ifold] = {\n",
        "        \"model_state\": model_state,\n",
        "        \"scores_train\": scores_train,\n",
        "        \"scores_val\": scores_val,\n",
        "        \"scores_test\": scores_test,\n",
        "    }\n",
        "\n",
        "    df_MD = pd.DataFrame({\n",
        "        \"score\": np.r_[scores_train.numpy(), scores_val.numpy(), scores_test.numpy()],\n",
        "        \"label\": ([\"train\"] * len(scores_train) + [\"val\"] * len(scores_val) + dev_test_loader.dataset.labels),\n",
        "    })\n",
        "\n",
        "    # スコアを対数変換\n",
        "    df_MD[\"log_score\"] = np.log1p(df_MD[\"score\"])\n",
        "\n",
        "    # ヒストグラムの描画\n",
        "    fig = px.histogram(\n",
        "        df_MD, x=\"log_score\", color=\"label\", nbins=30, histnorm=None, opacity=0.6\n",
        "    )\n",
        "    fig.update_layout(barmode=\"overlay\", title=f\"Mahalanobis Distances (Fold {ifold})\")\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0808f484",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 各フォールドのvalデータのスコアのFPR=1%点を計算\n",
        "folds_results_MD = []\n",
        "for ifold, results in all_results_MD.items():\n",
        "    scores_val = results[\"scores_val\"].numpy()\n",
        "    scores_test = results[\"scores_test\"].numpy()\n",
        "    labels_val = np.array([0]*len(scores_val))  # valデータはすべて正常\n",
        "    labels_test = np.array([0 if lbl == \"good\" else 1 for lbl in dev_test_loader.dataset.labels])  # testデータのラベル\n",
        "\n",
        "    # valデータで閾値を決定（FPR=1%点）\n",
        "    threshold = np.percentile(scores_val, 99)  # 上位1%を異常とする閾値\n",
        "\n",
        "    # testデータでの異常検知結果を計算\n",
        "    preds_test = (scores_test >= threshold).astype(int)\n",
        "\n",
        "    # 評価指標を計算\n",
        "    auc = roc_auc_score(labels_test, scores_test)\n",
        "    f1 = f1_score(labels_test, preds_test)\n",
        "    folds_results_MD.append({\n",
        "        \"fold\": ifold,\n",
        "        \"threshold\": threshold,\n",
        "        \"auc\": auc,\n",
        "        \"f1\": f1\n",
        "    })\n",
        "    print(f\"[Fold {ifold}] Val threshold (FPR=1%): {threshold:.2f}, Test AUC: {auc:.4f}, F1: {f1:.4f}\")\n",
        "# AUC, F1 の平均と標準偏差を計算\n",
        "aucs = [r[\"auc\"] for r in folds_results_MD]\n",
        "f1s = [r[\"f1\"] for r in folds_results_MD]\n",
        "print(f\"[Mahalanobis] Test AUC: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}, F1: {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "818d2ac9",
      "metadata": {},
      "source": [
        "### PaDiMでの異常検知実験"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d649ee79",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import importlib, sys\n",
        "import anomaly_core  # パス確認\n",
        "print(\"anomaly_core path:\", anomaly_core)\n",
        "\n",
        "importlib.invalidate_caches()\n",
        "importlib.reload(anomaly_core)\n",
        "\n",
        "# 関数を再インポート（← これが大事）\n",
        "\n",
        "from anomaly_core import fit_padim, padim_heatmap, all_padim_scores, fit_mahalanobis, all_mahalanobis_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b6c079",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CV 各フォールドで PaDiM を学習・評価\n",
        "all_results_PaDiM = {}\n",
        "for ifold, fold in enumerate(cv_folds):\n",
        "    model_state = fit_padim(\n",
        "        fold[\"train_loader\"],\n",
        "        backbone,\n",
        "        layers=padim_layers,\n",
        "        d=padim_channel_subsample,\n",
        "        device=device\n",
        "        )\n",
        "    scores_train = all_padim_scores(model_state, fold[\"train_loader\"])\n",
        "    scores_val = all_padim_scores(model_state, fold[\"val_loader\"])\n",
        "    scores_test, heatmaps_test = all_padim_scores(model_state, dev_test_loader, return_maps=True)\n",
        "\n",
        "    all_results_PaDiM[ifold] = {\n",
        "        \"model_state\": model_state,\n",
        "        \"scores_train\": scores_train,\n",
        "        \"scores_val\": scores_val,\n",
        "        \"scores_test\": scores_test,\n",
        "        \"heatmaps_test\": heatmaps_test,\n",
        "    }\n",
        "\n",
        "    df_PaDiM = pd.DataFrame({\n",
        "        \"score\": np.r_[scores_train.numpy(), scores_val.numpy(), scores_test.numpy()],\n",
        "        \"label\": ([\"train\"] * len(scores_train) + [\"val\"] * len(scores_val) + dev_test_loader.dataset.labels),\n",
        "    })\n",
        "\n",
        "    # スコアを対数変換\n",
        "    df_PaDiM[\"log_score\"] = np.log1p(df_PaDiM[\"score\"])\n",
        "\n",
        "    # ヒストグラムの描画\n",
        "    fig = px.histogram(\n",
        "        df_PaDiM, x=\"log_score\", color=\"label\", nbins=30, histnorm=None, opacity=0.6\n",
        "    )\n",
        "    fig.update_layout(barmode=\"overlay\", title=f\"PaDiM Scores (Fold {ifold})\")\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0131629",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 評価指標の算出\n",
        "folds_results_PaDiM = []\n",
        "for ifold, results in all_results_PaDiM.items():\n",
        "    scores_val = results[\"scores_val\"].numpy()\n",
        "    scores_test = results[\"scores_test\"].numpy()\n",
        "    labels_val = np.zeros_like(scores_val)  # valは正常のみ\n",
        "    labels_test = np.array([0 if lbl == \"good\" else 1 for lbl in dev_test_loader.dataset.labels])\n",
        "\n",
        "    threshold = np.percentile(scores_val, 99)\n",
        "    preds_test = (scores_test >= threshold).astype(int)\n",
        "    auc = roc_auc_score(labels_test, scores_test)\n",
        "    f1 = f1_score(labels_test, preds_test)\n",
        "    folds_results_PaDiM.append({\n",
        "        \"fold\": ifold,\n",
        "        \"threshold\": threshold,\n",
        "        \"auc\": auc,\n",
        "        \"f1\": f1\n",
        "    })\n",
        "    print(\n",
        "        f\"[PaDiM Fold {ifold}] Val threshold (FPR=1%): {threshold:.4f}, \"\n",
        "        f\"Test AUC: {auc:.4f}, F1: {f1:.4f}\"\n",
        "    )\n",
        "# AUC, F1 の平均と標準偏差を計算\n",
        "aucs = [r[\"auc\"] for r in folds_results_PaDiM]\n",
        "f1s = [r[\"f1\"] for r in folds_results_PaDiM]\n",
        "print(f\"[PaDiM] Test AUC: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}, F1: {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0147491",
      "metadata": {},
      "source": [
        "#### PaDiMのヒートマップ表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7654b1c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def inv_transform(img_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    \"\"\"正規化済み画像テンソルを逆変換してNumPy配列にする\"\"\"\n",
        "    # バッチ次元がある場合は除去\n",
        "    if img_tensor.dim() == 4:\n",
        "        img = img_tensor.squeeze(0)\n",
        "    else:\n",
        "        img = img_tensor\n",
        "\n",
        "    # テンソルを NumPy 配列に変換（形状は [3, H, W]）\n",
        "    img_np = img.cpu().numpy()\n",
        "\n",
        "    # 逆正規化: 各チャネルについて (x * std + mean)\n",
        "    mean = np.array(mean)[:, None, None]\n",
        "    std = np.array(std)[:, None, None]\n",
        "    img_np = img_np * std + mean\n",
        "\n",
        "    # 値を [0, 1] にクリップ\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "    # 軸の順番を [3, H, W] → [H, W, 3] に変換\n",
        "    img_np = np.transpose(img_np, (1, 2, 0))\n",
        "\n",
        "    return img_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ffe81c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# テスト画像のヒートマップ表示（最初の3枚）\n",
        "fig, axes = plt.subplots(3, 2, figsize=(6, 9))\n",
        "for i in range(3):\n",
        "    img, lbl = dev_test_loader.dataset[i]\n",
        "    heatmap = all_results_PaDiM[0][\"heatmaps_test\"][i].numpy()\n",
        "    img = inv_transform(img)\n",
        "    axes[i, 0].imshow(img)\n",
        "    axes[i, 0].set_title(lbl)\n",
        "    axes[i, 0].axis(\"off\")\n",
        "    axes[i, 1].imshow(heatmap, cmap=\"hot\")\n",
        "    axes[i, 1].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e009ea6c",
      "metadata": {},
      "source": [
        "## 設定の保存\n",
        "出力先：assets/fixed_pipeline.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c5ab4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# デフォルトではファイルを書き出さない（テンプレートのため）。\n",
        "# 実際に保存したい場合は SAVE_FIXED=True にして実行してください。\n",
        "SAVE_FIXED = True\n",
        "\n",
        "fixed_pipeline = {\n",
        "    \"common\": {\"image_size\": image_size, \"seeds\": seeds},\n",
        "    \"algorithms\": {\"mahalanobis\": use_mahalanobis, \"padim\": use_padim},\n",
        "    \"threshold\": {\"method\": \"percentile\", \"percentile\": 99},\n",
        "    \"mahalanobis\": {\"backbone\": backbone, \"cov_estimator\": cov_estimator},\n",
        "    \"padim\": {\"backbone\": backbone, \"layers\": padim_layers, \"d\": padim_channel_subsample}\n",
        "}\n",
        "\n",
        "assets_dir = Path(\"assets\")\n",
        "assets_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg_path = assets_dir / \"fixed_pipeline.json\"\n",
        "\n",
        "if SAVE_FIXED:\n",
        "    with cfg_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fixed_pipeline, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"[INFO] Saved: {cfg_path}\")\n",
        "else:\n",
        "    print(\"[INFO] SAVE_FIXED=False のためファイルは出力しません。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 次の手順\n",
        "- 上記のテンプレート関数に実装を追加し、dev の test から閾値を決めて `SAVE_FIXED=True` で JSON を保存。\n",
        "- その後 `02_evaluation_report.ipynb` で eval カテゴリを一発評価。\n",
        "- リーク防止のため、02 ではパラメータ・閾値を変更しないこと。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "image-anomaly-detection",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
