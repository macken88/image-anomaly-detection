{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_experiments_dev — MVTec 画像異常検知（開発\n",
    "\n",
    "本ノートは dev カテゴリのみで設計を確定し、固定パイプライン設定を `assets/fixed_pipeline.json` に出力するためのテンプレートです。\n",
    "- データ取得は anomalib を用いる（AGENTS.md 準拠）\n",
    "- 手法は Mahalanobis / PaDiM を比較\n",
    "- 閾値は dev の test で画像レベル FPR=1% を目標に決定\n",
    "\n",
    "実行順序：Header → Data → Methods → Results → Save JSON/Artifacts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境・依存の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考: anomaly_detection.ipynb からの初期インポートを整理\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from sklearn.covariance import ledoit_wolf\n",
    "import matplotlib.pyplot as plt\n",
    "# 可視化などのユーティリティは必要に応じて追加\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ取得（anomalib 経由）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENTS.md: 既存の MVTEC_ROOT または data/mvtec を使用。\n",
    "# 未検出の場合は anomalib によりダウンロード。\n",
    "MVTEC_ROOT = Path(os.environ.get(\"MVTEC_ROOT\", \"data/mvtec\"))\n",
    "MVTEC_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# anomalib のAPIはバージョンで異なる可能性があるため、例示的に記述。\n",
    "# 実環境の anomalib バージョンに合わせて import と引数を調整してください。\n",
    "try:\n",
    "    from anomalib.data import MVTecAD\n",
    "    datamodule = MVTecAD(root=str(MVTEC_ROOT))\n",
    "    datamodule.prepare_data()  # download if needed\n",
    "    datamodule.setup()\n",
    "except Exception as e:\n",
    "    print(\"[WARN] anomalib のデータ取得セットアップで問題が発生しました。バージョンや引数を確認してください:\\n\", e)\n",
    "\n",
    "assert MVTEC_ROOT.exists(), \"MVTec root not found after anomalib setup.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験設定（dev のみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev カテゴリと seed を定義\n",
    "dev_category = \"carpet\"  # 例: AGENTS.md 推奨例\n",
    "seeds = [0, 1, 2]\n",
    "image_size = 256\n",
    "\n",
    "# 比較する手法（最小構成）\n",
    "use_mahalanobis = True\n",
    "use_padim = True\n",
    "\n",
    "# PaDiM や Mahalanobis で用いる backbone/layers 等は仮パラメータ（要調整）\n",
    "backbone = \"resnet18\"\n",
    "padim_layers = [\"layer2\", \"layer3\"]\n",
    "padim_channel_subsample = 100\n",
    "cov_estimator = \"ledoit_wolf\"  # Mahalanobis 用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods — Mahalanobis / PaDiM（テンプレート）\n",
    "- ここで特徴抽出（ImageNet 事前学習）/ 統計量推定 / 推論スコア化を実装します。\n",
    "- 本テンプレートでは骨子のみを用意しています。必要に応じて `anomaly_detection.ipynb` の実装を移植してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mahalanobis(train_loader, backbone=backbone, cov_estimator=cov_estimator):\n",
    "    \"\"\"Fit Mahalanobis model from training data.\"\"\"\n",
    "    model = models.__dict__[backbone](pretrained=True)\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "    feature_extractor.eval()\n",
    "    feats = []\n",
    "    device = next(feature_extractor.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            feat = feature_extractor(images).view(images.size(0), -1)\n",
    "            feats.append(feat.cpu().numpy())\n",
    "    feats = np.concatenate(feats, axis=0)\n",
    "    mean = feats.mean(axis=0)\n",
    "    cov, _ = ledoit_wolf(feats)\n",
    "    precision = np.linalg.pinv(cov)\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"precision\": precision,\n",
    "        \"feature_extractor\": feature_extractor,\n",
    "        \"meta\": {\"backbone\": backbone, \"cov_estimator\": cov_estimator},\n",
    "    }\n",
    "\n",
    "\n",
    "def score_mahalanobis(model_state, batch):\n",
    "    \"\"\"Return Mahalanobis distances for a batch.\"\"\"\n",
    "    feature_extractor = model_state[\"feature_extractor\"]\n",
    "    device = next(feature_extractor.parameters()).device\n",
    "    mean = torch.tensor(model_state[\"mean\"], device=device)\n",
    "    precision = torch.tensor(model_state[\"precision\"], device=device)\n",
    "    feature_extractor.eval()\n",
    "    images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = feature_extractor(images).view(images.size(0), -1)\n",
    "    diff = feats - mean\n",
    "    scores = torch.sqrt(torch.sum((diff @ precision) * diff, dim=1))\n",
    "    return scores.cpu()\n",
    "\n",
    "\n",
    "class _PadimFeatureExtractor(nn.Module):\n",
    "    \"\"\"Utility to capture intermediate feature maps.\"\"\"\n",
    "\n",
    "    def __init__(self, backbone, layers):\n",
    "        super().__init__()\n",
    "        self.model = models.__dict__[backbone](pretrained=True)\n",
    "        self.layers = layers\n",
    "        self.outputs = {}\n",
    "        for name, module in self.model.named_children():\n",
    "            if name in layers:\n",
    "                module.register_forward_hook(self._save_output(name))\n",
    "\n",
    "    def _save_output(self, name):\n",
    "        def hook(module, inp, out):\n",
    "            self.outputs[name] = out\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.outputs = {}\n",
    "        _ = self.model(x)\n",
    "        return [self.outputs[l] for l in self.layers]\n",
    "\n",
    "\n",
    "def fit_padim(train_loader, backbone=backbone, layers=padim_layers, d=padim_channel_subsample):\n",
    "    \"\"\"Fit PaDiM model and return per-location statistics.\"\"\"\n",
    "    feature_extractor = _PadimFeatureExtractor(backbone, layers)\n",
    "    feature_extractor.eval()\n",
    "    embedding_list = []\n",
    "    device = next(feature_extractor.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            feats = feature_extractor(images)\n",
    "            feats = [\n",
    "                F.interpolate(f, size=feats[0].shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                for f in feats\n",
    "            ]\n",
    "            embedding = torch.cat(feats, dim=1)\n",
    "            embedding_list.append(embedding.cpu())\n",
    "    embeddings = torch.cat(embedding_list, dim=0)\n",
    "    c = embeddings.shape[1]\n",
    "    h, w = embeddings.shape[2:]\n",
    "    torch.manual_seed(0)\n",
    "    idx = torch.randperm(c)[:d]\n",
    "    embeddings = embeddings[:, idx, :, :]\n",
    "    embeddings = embeddings.permute(0, 2, 3, 1).reshape(-1, h * w, d)\n",
    "    mean = embeddings.mean(dim=0)\n",
    "    cov = torch.zeros(h * w, d, d)\n",
    "    for i in range(h * w):\n",
    "        cov[i] = torch.from_numpy(np.cov(embeddings[:, i, :].T))\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"cov\": cov,\n",
    "        \"idx\": idx,\n",
    "        \"feature_extractor\": feature_extractor,\n",
    "        \"meta\": {\"backbone\": backbone, \"layers\": layers, \"d\": d},\n",
    "    }\n",
    "\n",
    "\n",
    "def score_padim(model_state, batch):\n",
    "    \"\"\"Return PaDiM anomaly scores for batch.\"\"\"\n",
    "    feature_extractor = model_state[\"feature_extractor\"]\n",
    "    device = next(feature_extractor.parameters()).device\n",
    "    mean = model_state[\"mean\"].to(device)\n",
    "    cov = model_state[\"cov\"].to(device)\n",
    "    idx = model_state[\"idx\"]\n",
    "    feature_extractor.eval()\n",
    "    images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = feature_extractor(images)\n",
    "        feats = [\n",
    "            F.interpolate(f, size=feats[0].shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            for f in feats\n",
    "        ]\n",
    "        embedding = torch.cat(feats, dim=1)[:, idx, :, :]\n",
    "    n, d, h, w = embedding.shape\n",
    "    embedding = embedding.permute(0, 2, 3, 1).reshape(n, h * w, d)\n",
    "    scores = []\n",
    "    for emb in embedding:\n",
    "        dist = []\n",
    "        for i in range(h * w):\n",
    "            diff = emb[i] - mean[i]\n",
    "            inv = torch.linalg.pinv(cov[i])\n",
    "            dist.append(torch.sqrt(diff @ inv @ diff))\n",
    "        dist = torch.stack(dist).reshape(h, w)\n",
    "        scores.append(dist.max())\n",
    "    return torch.stack(scores).cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results — Cross-Validation & Metrics（テンプレート）\n",
    "- 各手法で dev の train を用いたクロスバリデーションを実施し、訓練内/訓練外スコアのヒストグラムを確認する。\n",
    "- dev の test で閾値と評価指標（AUROC, F1 など）の関係を可視化する。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: implement cross-validation loops and metric computation\n",
    "# for seed in seeds:\n",
    "#     # split dev train into folds\n",
    "#     # fit models and compute scores\n",
    "#     # plot histograms and metric curves\n",
    "#     pass\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 閾値決定（FPR=1% 目標、dev の test のみ）"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: ここでは閾値決定の骨子のみを用意。\n",
    "# 実装時は dev の test スコア分布から FPR=1% となるスコアを求めてください。\n",
    "image_fpr_target = 0.01\n",
    "threshold_value = None  # TODO: compute from dev test scores\n",
    "threshold_source = f\"dev_{dev_category}_test\"\n",
    "print(\"[INFO] threshold_source:\", threshold_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定パイプラインの保存（assets/fixed_pipeline.json）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デフォルトではファイルを書き出さない（テンプレートのため）。\n",
    "# 実際に保存したい場合は SAVE_FIXED=True にして実行してください。\n",
    "SAVE_FIXED = False\n",
    "\n",
    "fixed_pipeline = {\n",
    "    \"common\": {\"image_size\": image_size, \"seeds\": seeds},\n",
    "    \"threshold\": {\"image_fpr_target\": image_fpr_target, \"value\": threshold_value, \"source\": threshold_source},\n",
    "    \"mahalanobis\": {\"backbone\": backbone, \"cov_estimator\": cov_estimator},\n",
    "    \"padim\": {\"layers\": padim_layers, \"channel_subsample\": padim_channel_subsample}\n",
    "}\n",
    "\n",
    "assets_dir = Path(\"assets\")\n",
    "assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg_path = assets_dir / \"fixed_pipeline.json\"\n",
    "\n",
    "if SAVE_FIXED:\n",
    "    with cfg_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(fixed_pipeline, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"[INFO] Saved: {cfg_path}\")\n",
    "else:\n",
    "    print(\"[INFO] SAVE_FIXED=False のためファイルは出力しません。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次の手順\n",
    "- 上記のテンプレート関数に実装を追加し、dev の test から閾値を決めて `SAVE_FIXED=True` で JSON を保存。\n",
    "- その後 `02_evaluation_report.ipynb` で eval カテゴリを一発評価。\n",
    "- リーク防止のため、02 ではパラメータ・閾値を変更しないこと。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}