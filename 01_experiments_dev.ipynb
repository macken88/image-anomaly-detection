{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01_experiments_dev — MVTec 画像異常検知（開発\n",
        "\n",
        "本ノートは dev カテゴリのみで設計を確定し、固定パイプライン設定を `assets/fixed_pipeline.json` に出力するためのテンプレートです。\n",
        "- データ取得は anomalib を用いる（AGENTS.md 準拠）\n",
        "- 手法は Mahalanobis / PaDiM を比較\n",
        "- 閾値は dev の test で画像レベル FPR=1% を目標に決定\n",
        "\n",
        "実行順序：Header → Data → Methods → Results → Save JSON/Artifacts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 環境・依存の読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5768fe73",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "from sklearn.covariance import ledoit_wolf\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import warnings\n",
        "\n",
        "# anomaly_detectors からコア関数をインポート\n",
        "from anomaly_detectors import (\n",
        "    fit_mahalanobis, all_mahalanobis_scores,\n",
        "    fit_padim, padim_heatmap, all_padim_scores,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65088a9b",
      "metadata": {},
      "source": [
        "## データ取得（anomalib 経由）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "01228e6e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 既存の MVTEC_ROOT または datasets/MVTecAD を使用。\n",
        "# 未検出の場合は anomalib によりダウンロード。\n",
        "MVTEC_ROOT = Path(os.environ.get(\"MVTEC_ROOT\", \"datasets/MVTecAD\"))\n",
        "MVTEC_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# anomalib のAPIはバージョンで異なる可能性があるため、例示的に記述。\n",
        "# 実環境の anomalib バージョンに合わせて import と引数を調整してください。\n",
        "try:\n",
        "    from anomalib.data import MVTecAD\n",
        "    datamodule = MVTecAD(root=str(MVTEC_ROOT))\n",
        "    datamodule.prepare_data()  # download if needed\n",
        "    datamodule.setup()\n",
        "except Exception as e:\n",
        "    print(\"[WARN] anomalib のデータ取得セットアップで問題が発生しました。バージョンや引数を確認してください:\\n\", e)\n",
        "\n",
        "assert MVTEC_ROOT.exists(), \"MVTec root not found after anomalib setup.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe101f49",
      "metadata": {},
      "source": [
        "## 実験設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "44d07880",
      "metadata": {},
      "outputs": [],
      "source": [
        "# torchvision のモデル名から学習済みモデルを読み込む関数\n",
        "def load_backbone_from_name(name: str):\n",
        "    try:\n",
        "        from torchvision.models import get_model, get_model_weights\n",
        "        weights = None\n",
        "        try:\n",
        "            weights = get_model_weights(name).DEFAULT  # 学習済みウェイト\n",
        "        except Exception:\n",
        "            pass  # ウェイトが無いモデルはランダム初期化で作る\n",
        "        return get_model(name, weights=weights).eval()\n",
        "    except Exception:\n",
        "        # 旧 API フォールバック（古い torchvision 向け）\n",
        "        if not hasattr(models, name):\n",
        "            raise ValueError(f\"Unknown backbone name: {name}\")\n",
        "        return models.__dict__[name](pretrained=True).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d49852b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# dev カテゴリを定義\n",
        "dev_category = \"carpet\" \n",
        "\n",
        "# モデル本体インポート\n",
        "image_size = 256\n",
        "threshold_percentile = 99\n",
        "#backbone = \"resnet18\" # \"efficientnet_b0\"\n",
        "#model = load_backbone_from_name(backbone)\n",
        "\n",
        "# efficientnet_b0の場合\n",
        "#try:\n",
        "#    from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "#    model = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT).eval()\n",
        "#except Exception:\n",
        "#    model = models.efficientnet_b0(pretrained=True).eval()\n",
        "#backbone = model._get_name()\n",
        "#MD_layer = 'flatten'\n",
        "#padim_layers = ['features.6.3.add', 'features.7.0.block.0', 'features.7.0.block.1', 'features.7.0.block.2', 'features.7.0.block.3']\n",
        "\n",
        "# resnet18の場合\n",
        "try:\n",
        "    from torchvision.models import resnet18, ResNet18_Weights\n",
        "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1).eval() #IMAGENET1K_V1\n",
        "except Exception:\n",
        "    model = models.resnet18(pretrained=True).eval()\n",
        "backbone = model._get_name()\n",
        "MD_layer = 'layer4.1.relu_1'\n",
        "padim_layers = [\"layer1.1.relu_1\", \"layer2.1.relu_1\", \"layer3.1.relu_1\"]\n",
        "\n",
        "padim_channel_subsample = 100\n",
        "\n",
        "EXPERIMENT_NAME = f\"{backbone}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b74d370c",
      "metadata": {},
      "source": [
        "## 学習と推論\n",
        "- 各手法で dev の train を用いたクロスバリデーションを実施し、訓練内/訓練外スコアのヒストグラムを確認する。\n",
        "- dev の test で閾値と評価指標（AUROC, F1 など）の関係を可視化する。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da5dc09a",
      "metadata": {},
      "source": [
        "### データローダー準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68fb8377",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Category: carpet\n",
            "[INFO] CV folds: 5\n",
            "  - fold 0: n_train=224, n_val=56\n",
            "  - fold 1: n_train=224, n_val=56\n",
            "  - fold 2: n_train=224, n_val=56\n",
            "  - fold 3: n_train=224, n_val=56\n",
            "  - fold 4: n_train=224, n_val=56\n",
            "[INFO] Dev test size: 117\n",
            "[INFO] Test label distribution: {'color': 19, 'cut': 17, 'good': 28, 'hole': 17, 'metal_contamination': 17, 'thread': 19}\n"
          ]
        }
      ],
      "source": [
        "# Data loading for cross-validation (dev train) and dev test\n",
        "# - Builds KFold train/val DataLoaders using only train/good images.\n",
        "# - Prepares dev test DataLoader with labels (good=0, defect=1).\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Any\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import os\n",
        "\n",
        "# Transforms (ImageNet mean/std)\n",
        "imagenet_mean = [0.485, 0.456, 0.406]\n",
        "imagenet_std = [0.229, 0.224, 0.225]\n",
        "_transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
        "])\n",
        "\n",
        "class ImagePathDataset(Dataset):\n",
        "    \"\"\"Minimal dataset returning (image_tensor, label).\n",
        "\n",
        "    Paths: list of filesystem paths; Labels: list[Any] of same length.\n",
        "    \"\"\"\n",
        "    def __init__(self, paths: List[Path], labels: List[Any], transform=None):\n",
        "        self.paths = [Path(p) for p in paths]\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.paths[idx]\n",
        "        y = self.labels[idx]\n",
        "        img = Image.open(p).convert(\"RGB\")\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, y\n",
        "\n",
        "def _existing_category_root(category: str) -> Path:\n",
        "    \"\"\"Find an existing MVTec category root among common layouts.\n",
        "    Prefers MVTEC_ROOT, then 'datasets/MVTecAD', then 'MVtec_dataset'.\n",
        "    \"\"\"\n",
        "    candidates = [\n",
        "        MVTEC_ROOT / category,\n",
        "        Path(\"datasets/MVTecAD\") / category,\n",
        "        Path(\"MVtec_dataset\") / category,\n",
        "    ]\n",
        "    for c in candidates:\n",
        "        if c.exists():\n",
        "            return c\n",
        "    raise FileNotFoundError(f\"MVTec category not found: {category}\")\n",
        "\n",
        "def _list_images(d: Path) -> List[Path]:\n",
        "    exts = {\".png\", \".jpg\", \".jpeg\"}\n",
        "    if not d.exists():\n",
        "        return []\n",
        "    return sorted([p for p in d.rglob('*') if p.suffix.lower() in exts])\n",
        "\n",
        "def build_cv_and_test_loaders(category: str, k_splits: int = 5, batch_size: int = 32) -> Tuple[list, DataLoader]:\n",
        "    \"\"\"Return (cv_folds, test_loader).\n",
        "\n",
        "    cv_folds: list of dicts with 'train_loader' and 'val_loader'.\n",
        "    test_loader: dev test DataLoader with labels equal to defect types\n",
        "                 (the directory names directly under 'test', e.g., 'good',\n",
        "                 'scratch', 'hole', ...).\n",
        "    \"\"\"\n",
        "    root = _existing_category_root(category)\n",
        "    train_good = _list_images(root / 'train' / 'good')\n",
        "    assert len(train_good) > 0, f\"No train/good images found for {category}\"\n",
        "\n",
        "    # Prepare KFold over indices (all labels are 0 in train).\n",
        "    kf = KFold(n_splits=k_splits, shuffle=True, random_state=0)\n",
        "    base_ds = ImagePathDataset(train_good, [0] * len(train_good), transform=_transform)\n",
        "\n",
        "    num_workers = min(4, os.cpu_count() or 1)\n",
        "    cv_folds = []\n",
        "    for fold_id, (tr_idx, va_idx) in enumerate(kf.split(range(len(train_good)))):\n",
        "        tr_ds = Subset(base_ds, tr_idx)\n",
        "        va_ds = Subset(base_ds, va_idx)\n",
        "        tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "        va_loader = DataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "        cv_folds.append({\n",
        "            'fold': fold_id,\n",
        "            'train_loader': tr_loader,\n",
        "            'val_loader': va_loader,\n",
        "            'n_train': len(tr_idx),\n",
        "            'n_val': len(va_idx),\n",
        "        })\n",
        "\n",
        "    # Build dev test loader with labels as defect types (dir names under 'test').\n",
        "    test_dir = root / 'test'\n",
        "    test_paths: List[Path] = []\n",
        "    test_labels: List[str] = []\n",
        "    if test_dir.exists():\n",
        "        # Iterate over subdirectories directly under 'test' (including 'good').\n",
        "        for sub in sorted([d for d in test_dir.iterdir() if d.is_dir()], key=lambda p: p.name):\n",
        "            label = sub.name  # defect type (or 'good')\n",
        "            paths = _list_images(sub)\n",
        "            if paths:\n",
        "                test_paths.extend(paths)\n",
        "                test_labels.extend([label] * len(paths))\n",
        "    assert len(test_paths) > 0, f\"No test images found for {category}\"\n",
        "\n",
        "    test_ds = ImagePathDataset(test_paths, test_labels, transform=_transform)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "    return cv_folds, test_loader\n",
        "\n",
        "# Build loaders for the chosen dev category\n",
        "cv_folds, dev_test_loader = build_cv_and_test_loaders(dev_category, k_splits=5, batch_size=32)\n",
        "print(f\"[INFO] Category: {dev_category}\")\n",
        "print(f\"[INFO] CV folds: {len(cv_folds)}\")\n",
        "for f in cv_folds:\n",
        "    print(f\"  - fold {f['fold']}: n_train={f['n_train']}, n_val={f['n_val']}\")\n",
        "from collections import Counter\n",
        "cnt = Counter(dev_test_loader.dataset.labels)\n",
        "print(f\"[INFO] Dev test size: {len(dev_test_loader.dataset)}\")\n",
        "print(f\"[INFO] Test label distribution: {dict(cnt)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "fbb268b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"[INFO] Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "results_setup",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Results will be saved under: results/dev/202509201455_ResNet\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "ts = datetime.now().strftime('%Y%m%d%H%M')\n",
        "base_dir = Path(f'results/dev/{ts}_{EXPERIMENT_NAME}')\n",
        "dir_md = base_dir / 'MD'\n",
        "dir_padim = base_dir / 'PaDiM'\n",
        "dir_md.mkdir(parents=True, exist_ok=True); dir_padim.mkdir(parents=True, exist_ok=True)\n",
        "print(f'[INFO] Results will be saved under: {base_dir}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "288a6651",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_histogram(df: pd.DataFrame, ifold: int, save_dir: Path):\n",
        "    # スコアを対数変換\n",
        "    df[\"log_score\"] = np.log1p(df[\"score\"])\n",
        "\n",
        "    # ヒストグラムの描画（Matplotlib）\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "    # 全データでビンを共有（Plotlyのnbins=30相当）\n",
        "    n_bins = 30\n",
        "    bin_edges = np.linspace(df[\"log_score\"].min(), df[\"log_score\"].max(), n_bins + 1)\n",
        "\n",
        "    # 表示順を「train」「val」「good」（テストデータ）優先にし、残りは登場順で\n",
        "    labels_all = df[\"label\"].unique().tolist()\n",
        "    ordered = [\"train\", \"val\", \"good\"] + [l for l in labels_all if l not in (\"train\", \"val\", \"good\")]\n",
        "\n",
        "    for lb in ordered:\n",
        "        vals = df.loc[df[\"label\"] == lb, \"log_score\"].to_numpy()\n",
        "        if len(vals) == 0:\n",
        "            continue\n",
        "        ax.hist(\n",
        "            vals,\n",
        "            bins=bin_edges,\n",
        "            alpha=0.6,\n",
        "            label=lb,\n",
        "            edgecolor=\"black\",\n",
        "            linewidth=0.3,\n",
        "        )\n",
        "\n",
        "    ax.set_title(f\"Anomaly Scores (Fold {ifold})\")\n",
        "    ax.set_xlabel(\"log_score\")\n",
        "    ax.set_ylabel(\"count\")\n",
        "    # ラベル数が多い場合の視認性配慮（必要に応じて調整）\n",
        "    ax.legend(fontsize=8, ncol=2)\n",
        "    fig.tight_layout()\n",
        "\n",
        "    # 画面表示\n",
        "    plt.show()\n",
        "\n",
        "    # 保存（PNG）\n",
        "    out_png = save_dir / f\"fold_{ifold}_hist.png\"\n",
        "    fig.savefig(out_png, dpi=150)\n",
        "    print(f\"[INFO] Saved histogram PNG: {out_png}\")\n",
        "\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "031692ae",
      "metadata": {},
      "source": [
        "### マハラノビス距離ベースでの異常検知実験"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3a1b9b2",
      "metadata": {},
      "outputs": [],
      "source": [
        "with warnings.catch_warnings():\n",
        "    # UserWarningを非表示\n",
        "    warnings.filterwarnings('ignore', category=UserWarning) \n",
        "    \n",
        "    # CV 各フォールドで Mahalanobis を学習・評価\n",
        "    all_results_MD = {}\n",
        "    for ifold, fold in enumerate(cv_folds):\n",
        "        model_state = fit_mahalanobis(fold[\"train_loader\"], model, feature_node=MD_layer, device=device)\n",
        "        scores_train = all_mahalanobis_scores(model_state, fold[\"train_loader\"])\n",
        "        scores_val = all_mahalanobis_scores(model_state, fold[\"val_loader\"])\n",
        "        scores_test = all_mahalanobis_scores(model_state, dev_test_loader)\n",
        "\n",
        "        all_results_MD[ifold] = {\n",
        "            \"model_state\": model_state,\n",
        "            \"scores_train\": scores_train,\n",
        "            \"scores_val\": scores_val,\n",
        "            \"scores_test\": scores_test,\n",
        "        }\n",
        "\n",
        "        df_MD = pd.DataFrame({\n",
        "            \"score\": np.r_[scores_train.numpy(), scores_val.numpy(), scores_test.numpy()],\n",
        "            \"label\": ([\"train\"] * len(scores_train) + [\"val\"] * len(scores_val) + dev_test_loader.dataset.labels),\n",
        "        })\n",
        "\n",
        "        plot_histogram(df_MD, ifold, dir_md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0808f484",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 各フォールドのvalデータのスコアのFPR=1%点を計算\n",
        "folds_results_MD = []\n",
        "for ifold, results in all_results_MD.items():\n",
        "    scores_val = results[\"scores_val\"].numpy()\n",
        "    scores_test = results[\"scores_test\"].numpy()\n",
        "    labels_val = np.array([0]*len(scores_val))  # valデータはすべて正常\n",
        "    labels_test = np.array([0 if lbl == \"good\" else 1 for lbl in dev_test_loader.dataset.labels])  # testデータのラベル\n",
        "\n",
        "    # valデータで閾値を決定（FPR=1%点）\n",
        "    threshold = np.percentile(scores_val, threshold_percentile)  # 上位1%を異常とする閾値\n",
        "\n",
        "    # testデータでの異常検知結果を計算\n",
        "    preds_test = (scores_test >= threshold).astype(int)\n",
        "\n",
        "    # 評価指標を計算\n",
        "    auc = roc_auc_score(labels_test, scores_test)\n",
        "    f1 = f1_score(labels_test, preds_test)\n",
        "    folds_results_MD.append({\n",
        "        \"fold\": ifold,\n",
        "        \"threshold\": threshold,\n",
        "        \"auc\": auc,\n",
        "        \"f1\": f1\n",
        "    })\n",
        "    print(f\"[Fold {ifold}] Val threshold (FPR=1%): {threshold:.2f}, Test AUC: {auc:.4f}, F1: {f1:.4f}\")\n",
        "# AUC, F1 の平均と標準偏差を計算\n",
        "aucs = [r[\"auc\"] for r in folds_results_MD]\n",
        "f1s = [r[\"f1\"] for r in folds_results_MD]\n",
        "print(f\"[Mahalanobis] Test AUC: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}, F1: {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n",
        "\n",
        "# 保存（サマリーCSV: 各フォールド1行）\n",
        "df_md_summary = pd.DataFrame(folds_results_MD)\n",
        "csv_md = dir_md / 'summary.csv'\n",
        "df_md_summary.to_csv(csv_md, index=False)\n",
        "print(f'[INFO] Saved summary CSV: {csv_md}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "818d2ac9",
      "metadata": {},
      "source": [
        "### PaDiMでの異常検知実験"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b6c079",
      "metadata": {},
      "outputs": [],
      "source": [
        "with warnings.catch_warnings():\n",
        "    # UserWarningを非表示\n",
        "    warnings.filterwarnings('ignore', category=UserWarning) \n",
        "\n",
        "    # CV 各フォールドで PaDiM を学習・評価\n",
        "    all_results_PaDiM = {}\n",
        "    for ifold, fold in enumerate(cv_folds):\n",
        "        model_state = fit_padim(\n",
        "            fold[\"train_loader\"],\n",
        "            model,\n",
        "            layers=padim_layers,\n",
        "            d=padim_channel_subsample,\n",
        "            device=device\n",
        "            )\n",
        "        scores_train = all_padim_scores(model_state, fold[\"train_loader\"])\n",
        "        scores_val = all_padim_scores(model_state, fold[\"val_loader\"])\n",
        "        scores_test, heatmaps_test = all_padim_scores(model_state, dev_test_loader, return_maps=True)\n",
        "\n",
        "        all_results_PaDiM[ifold] = {\n",
        "            \"model_state\": model_state,\n",
        "            \"scores_train\": scores_train,\n",
        "            \"scores_val\": scores_val,\n",
        "            \"scores_test\": scores_test,\n",
        "            \"heatmaps_test\": heatmaps_test,\n",
        "        }\n",
        "\n",
        "        df_PaDiM = pd.DataFrame({\n",
        "            \"score\": np.r_[scores_train.numpy(), scores_val.numpy(), scores_test.numpy()],\n",
        "            \"label\": ([\"train\"] * len(scores_train) + [\"val\"] * len(scores_val) + dev_test_loader.dataset.labels),\n",
        "        })\n",
        "\n",
        "        plot_histogram(df_PaDiM, ifold, dir_padim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0131629",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 評価指標の算出\n",
        "folds_results_PaDiM = []\n",
        "for ifold, results in all_results_PaDiM.items():\n",
        "    scores_val = results[\"scores_val\"].numpy()\n",
        "    scores_test = results[\"scores_test\"].numpy()\n",
        "    labels_val = np.zeros_like(scores_val)  # valは正常のみ\n",
        "    labels_test = np.array([0 if lbl == \"good\" else 1 for lbl in dev_test_loader.dataset.labels])\n",
        "\n",
        "    threshold = np.percentile(scores_val, threshold_percentile)\n",
        "    preds_test = (scores_test >= threshold).astype(int)\n",
        "    auc = roc_auc_score(labels_test, scores_test)\n",
        "    f1 = f1_score(labels_test, preds_test)\n",
        "    folds_results_PaDiM.append({\n",
        "        \"fold\": ifold,\n",
        "        \"threshold\": threshold,\n",
        "        \"auc\": auc,\n",
        "        \"f1\": f1\n",
        "    })\n",
        "    print(\n",
        "        f\"[PaDiM Fold {ifold}] Val threshold (FPR=1%): {threshold:.4f}, \"\n",
        "        f\"Test AUC: {auc:.4f}, F1: {f1:.4f}\"\n",
        "    )\n",
        "# AUC, F1 の平均と標準偏差を計算\n",
        "aucs = [r[\"auc\"] for r in folds_results_PaDiM]\n",
        "f1s = [r[\"f1\"] for r in folds_results_PaDiM]\n",
        "print(f\"[PaDiM] Test AUC: {np.mean(aucs):.4f} ± {np.std(aucs):.4f}, F1: {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n",
        "\n",
        "# 保存（サマリーCSV: 各フォールド1行）\n",
        "df_p_summary = pd.DataFrame(folds_results_PaDiM)\n",
        "csv_p = dir_padim / 'summary.csv'\n",
        "df_p_summary.to_csv(csv_p, index=False)\n",
        "print(f'[INFO] Saved summary CSV: {csv_p}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0147491",
      "metadata": {},
      "source": [
        "#### PaDiMのヒートマップ表示"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7654b1c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def inv_transform(img_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    \"\"\"正規化済み画像テンソルを逆変換してNumPy配列にする\"\"\"\n",
        "    # バッチ次元がある場合は除去\n",
        "    if img_tensor.dim() == 4:\n",
        "        img = img_tensor.squeeze(0)\n",
        "    else:\n",
        "        img = img_tensor\n",
        "\n",
        "    # テンソルを NumPy 配列に変換（形状は [3, H, W]）\n",
        "    img_np = img.cpu().numpy()\n",
        "\n",
        "    # 逆正規化: 各チャネルについて (x * std + mean)\n",
        "    mean = np.array(mean)[:, None, None]\n",
        "    std = np.array(std)[:, None, None]\n",
        "    img_np = img_np * std + mean\n",
        "\n",
        "    # 値を [0, 1] にクリップ\n",
        "    img_np = np.clip(img_np, 0, 1)\n",
        "\n",
        "    # 軸の順番を [3, H, W] → [H, W, 3] に変換\n",
        "    img_np = np.transpose(img_np, (1, 2, 0))\n",
        "\n",
        "    return img_np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ffe81c7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# テスト画像からランダムに3枚を表示\n",
        "random.seed(0)\n",
        "indices = random.sample(range(len(dev_test_loader.dataset)), 3)\n",
        "heatmaps = [\n",
        "    (idx, all_results_PaDiM[0][\"heatmaps_test\"][idx].numpy())\n",
        "    for idx in indices\n",
        "]\n",
        "\n",
        "vmin = min(hm.min() for _, hm in heatmaps)\n",
        "vmax = max(hm.max() for _, hm in heatmaps)\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(6, 9))\n",
        "for row, (idx, heatmap) in enumerate(heatmaps):\n",
        "    img, lbl = dev_test_loader.dataset[idx]\n",
        "    img = inv_transform(img)\n",
        "    axes[row, 0].imshow(img)\n",
        "    axes[row, 0].set_title(lbl)\n",
        "    axes[row, 0].axis(\"off\")\n",
        "    axes[row, 1].imshow(heatmap, cmap=\"hot\", vmin=vmin, vmax=vmax)\n",
        "    axes[row, 1].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ヒートマップ画像の保存\n",
        "out_png = dir_padim / \"sample_heatmaps.png\"\n",
        "fig.savefig(out_png, dpi=150)\n",
        "print(f\"[INFO] Saved sample heatmaps PNG: {out_png}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e009ea6c",
      "metadata": {},
      "source": [
        "## 設定の保存\n",
        "出力先：assets/fixed_pipeline.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c5ab4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# デフォルトではファイルを書き出さない（テンプレートのため）。\n",
        "# 実際に保存したい場合は SAVE_FIXED=True にして実行してください。\n",
        "SAVE_FIXED = True\n",
        "\n",
        "fixed_pipeline = {\n",
        "    \"common\": {\n",
        "        \"image_size\": image_size,\n",
        "        \"backbone\": backbone,\n",
        "        \"threshold_percentile\": threshold_percentile\n",
        "    },\n",
        "    \"mahalanobis\": {\"layer\": MD_layer},\n",
        "    \"padim\": {\"layers\": padim_layers, \"d\": padim_channel_subsample}\n",
        "}\n",
        "\n",
        "assets_dir = Path(\"assets\")\n",
        "assets_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg_path = assets_dir / \"fixed_pipeline.json\"\n",
        "\n",
        "if SAVE_FIXED:\n",
        "    with cfg_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fixed_pipeline, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"[INFO] Saved: {cfg_path}\")\n",
        "else:\n",
        "    print(\"[INFO] SAVE_FIXED=False のため assets 固定ファイルはスキップ。\")\n",
        "\n",
        "# 必須: 実行設定を timestamp 付き出力ディレクトリにも保存\n",
        "cfg_out = base_dir / 'fixed_pipeline.json'\n",
        "with cfg_out.open('w', encoding='utf-8') as f:\n",
        "    json.dump(fixed_pipeline, f, indent=2, ensure_ascii=False)\n",
        "print(f\"[INFO] Saved config to: {cfg_out}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 次の手順\n",
        "- 上記のテンプレート関数に実装を追加し、dev の test から閾値を決めて `SAVE_FIXED=True` で JSON を保存。\n",
        "- その後 `02_evaluation_report.ipynb` で eval カテゴリを一発評価。\n",
        "- リーク防止のため、02 ではパラメータ・閾値を変更しないこと。\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "image-anomaly-detection",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
