{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_experiments_dev — MVTec 画像異常検知（開発\n",
    "\n",
    "本ノートは dev カテゴリのみで設計を確定し、固定パイプライン設定を `assets/fixed_pipeline.json` に出力するためのテンプレートです。\n",
    "- データ取得は anomalib を用いる（AGENTS.md 準拠）\n",
    "- 手法は Mahalanobis / PaDiM を比較\n",
    "- 閾値は dev の test で画像レベル FPR=1% を目標に決定\n",
    "\n",
    "実行順序：Header → Data → Methods → Results → Save JSON/Artifacts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 環境・依存の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考: anomaly_detection.ipynb からの初期インポートを整理\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from sklearn.covariance import ledoit_wolf\n",
    "import matplotlib.pyplot as plt\n",
    "# 可視化などのユーティリティは必要に応じて追加\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ取得（anomalib 経由）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGENTS.md: 既存の MVTEC_ROOT または datasets/MVTecAD を使用。\n",
    "# 未検出の場合は anomalib によりダウンロード。\n",
    "MVTEC_ROOT = Path(os.environ.get(\"MVTEC_ROOT\", \"datasets/MVTecAD\"))\n",
    "MVTEC_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# anomalib のAPIはバージョンで異なる可能性があるため、例示的に記述。\n",
    "# 実環境の anomalib バージョンに合わせて import と引数を調整してください。\n",
    "try:\n",
    "    from anomalib.data import MVTecAD\n",
    "    datamodule = MVTecAD(root=str(MVTEC_ROOT))\n",
    "    datamodule.prepare_data()  # download if needed\n",
    "    datamodule.setup()\n",
    "except Exception as e:\n",
    "    print(\"[WARN] anomalib のデータ取得セットアップで問題が発生しました。バージョンや引数を確認してください:\\n\", e)\n",
    "\n",
    "assert MVTEC_ROOT.exists(), \"MVTec root not found after anomalib setup.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験設定（dev のみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev カテゴリと seed を定義\n",
    "dev_category = \"carpet\"  # 例: AGENTS.md 推奨例\n",
    "seeds = [0, 1, 2]\n",
    "image_size = 256\n",
    "\n",
    "# 比較する手法（最小構成）\n",
    "use_mahalanobis = True\n",
    "use_padim = True\n",
    "\n",
    "# PaDiM や Mahalanobis で用いる backbone/layers 等は仮パラメータ（要調整）\n",
    "backbone = \"resnet18\"\n",
    "padim_layers = [\"layer2\", \"layer3\"]\n",
    "padim_channel_subsample = 100\n",
    "cov_estimator = \"ledoit_wolf\"  # Mahalanobis 用\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods — Mahalanobis / PaDiM（テンプレート）\n",
    "- ここで特徴抽出（ImageNet 事前学習）/ 統計量推定 / 推論スコア化を実装します。\n",
    "- 本テンプレートでは骨子のみを用意しています。必要に応じて `anomaly_detection.ipynb` の実装を移植してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_mahalanobis(train_loader, backbone=backbone, cov_estimator=cov_estimator):\n",
    "    \"\"\"Fit Mahalanobis model from training data.\"\"\"\n",
    "    model = models.__dict__[backbone](pretrained=True)\n",
    "    feature_extractor = nn.Sequential(*list(model.children())[:-1])\n",
    "    feature_extractor.eval()\n",
    "    feats = []\n",
    "    device = next(feature_extractor.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            feat = feature_extractor(images).view(images.size(0), -1)\n",
    "            feats.append(feat.cpu().numpy())\n",
    "    feats = np.concatenate(feats, axis=0)\n",
    "    mean = feats.mean(axis=0)\n",
    "    cov, _ = ledoit_wolf(feats)\n",
    "    precision = np.linalg.pinv(cov)\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"precision\": precision,\n",
    "        \"feature_extractor\": feature_extractor,\n",
    "        \"meta\": {\"backbone\": backbone, \"cov_estimator\": cov_estimator},\n",
    "    }\n",
    "\n",
    "\n",
    "def score_mahalanobis(model_state, batch):\n",
    "    \"\"\"Return Mahalanobis distances for a batch.\"\"\"\n",
    "    feature_extractor = model_state[\"feature_extractor\"]\n",
    "    device = next(feature_extractor.parameters()).device\n",
    "    mean = torch.tensor(model_state[\"mean\"], device=device)\n",
    "    precision = torch.tensor(model_state[\"precision\"], device=device)\n",
    "    feature_extractor.eval()\n",
    "    images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = feature_extractor(images).view(images.size(0), -1)\n",
    "    diff = feats - mean\n",
    "    scores = torch.sqrt(torch.sum((diff @ precision) * diff, dim=1))\n",
    "    return scores.cpu()\n",
    "\n",
    "\n",
    "class _PadimFeatureExtractor(nn.Module):\n",
    "    \"\"\"Utility to capture intermediate feature maps.\"\"\"\n",
    "\n",
    "    def __init__(self, backbone, layers):\n",
    "        super().__init__()\n",
    "        self.model = models.__dict__[backbone](pretrained=True)\n",
    "        self.layers = layers\n",
    "        self.outputs = {}\n",
    "        for name, module in self.model.named_children():\n",
    "            if name in layers:\n",
    "                module.register_forward_hook(self._save_output(name))\n",
    "\n",
    "    def _save_output(self, name):\n",
    "        def hook(module, inp, out):\n",
    "            self.outputs[name] = out\n",
    "        return hook\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.outputs = {}\n",
    "        _ = self.model(x)\n",
    "        return [self.outputs[l] for l in self.layers]\n",
    "\n",
    "\n",
    "def fit_padim(train_loader, backbone=backbone, layers=padim_layers, d=padim_channel_subsample):\n",
    "    \"\"\"Fit PaDiM model and return per-location statistics.\"\"\"\n",
    "    feature_extractor = _PadimFeatureExtractor(backbone, layers)\n",
    "    feature_extractor.eval()\n",
    "    embedding_list = []\n",
    "    device = next(feature_extractor.parameters()).device\n",
    "    with torch.no_grad():\n",
    "        for images, _ in train_loader:\n",
    "            images = images.to(device)\n",
    "            feats = feature_extractor(images)\n",
    "            feats = [\n",
    "                F.interpolate(f, size=feats[0].shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "                for f in feats\n",
    "            ]\n",
    "            embedding = torch.cat(feats, dim=1)\n",
    "            embedding_list.append(embedding.cpu())\n",
    "    embeddings = torch.cat(embedding_list, dim=0)\n",
    "    c = embeddings.shape[1]\n",
    "    h, w = embeddings.shape[2:]\n",
    "    torch.manual_seed(0)\n",
    "    idx = torch.randperm(c)[:d]\n",
    "    embeddings = embeddings[:, idx, :, :]\n",
    "    embeddings = embeddings.permute(0, 2, 3, 1).reshape(-1, h * w, d)\n",
    "    mean = embeddings.mean(dim=0)\n",
    "    cov = torch.zeros(h * w, d, d)\n",
    "    for i in range(h * w):\n",
    "        cov[i] = torch.from_numpy(np.cov(embeddings[:, i, :].T))\n",
    "    return {\n",
    "        \"mean\": mean,\n",
    "        \"cov\": cov,\n",
    "        \"idx\": idx,\n",
    "        \"feature_extractor\": feature_extractor,\n",
    "        \"meta\": {\"backbone\": backbone, \"layers\": layers, \"d\": d},\n",
    "    }\n",
    "\n",
    "\n",
    "def score_padim(model_state, batch):\n",
    "    \"\"\"Return PaDiM anomaly scores for batch.\"\"\"\n",
    "    feature_extractor = model_state[\"feature_extractor\"]\n",
    "    device = next(feature_extractor.parameters()).device\n",
    "    mean = model_state[\"mean\"].to(device)\n",
    "    cov = model_state[\"cov\"].to(device)\n",
    "    idx = model_state[\"idx\"]\n",
    "    feature_extractor.eval()\n",
    "    images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = feature_extractor(images)\n",
    "        feats = [\n",
    "            F.interpolate(f, size=feats[0].shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "            for f in feats\n",
    "        ]\n",
    "        embedding = torch.cat(feats, dim=1)[:, idx, :, :]\n",
    "    n, d, h, w = embedding.shape\n",
    "    embedding = embedding.permute(0, 2, 3, 1).reshape(n, h * w, d)\n",
    "    scores = []\n",
    "    for emb in embedding:\n",
    "        dist = []\n",
    "        for i in range(h * w):\n",
    "            diff = emb[i] - mean[i]\n",
    "            inv = torch.linalg.pinv(cov[i])\n",
    "            dist.append(torch.sqrt(diff @ inv @ diff))\n",
    "        dist = torch.stack(dist).reshape(h, w)\n",
    "        scores.append(dist.max())\n",
    "    return torch.stack(scores).cpu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results — Cross-Validation & Metrics（テンプレート）\n",
    "- 各手法で dev の train を用いたクロスバリデーションを実施し、訓練内/訓練外スコアのヒストグラムを確認する。\n",
    "- dev の test で閾値と評価指標（AUROC, F1 など）の関係を可視化する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Category: carpet\n",
      "[INFO] CV folds: 5\n",
      "  - fold 0: n_train=224, n_val=56\n",
      "  - fold 1: n_train=224, n_val=56\n",
      "  - fold 2: n_train=224, n_val=56\n",
      "  - fold 3: n_train=224, n_val=56\n",
      "  - fold 4: n_train=224, n_val=56\n",
      "[INFO] Dev test size: 117 (good=28, defect=89)\n"
     ]
    }
   ],
   "source": [
    "# Data loading for cross-validation (dev train) and dev test\n",
    "# - Builds KFold train/val DataLoaders using only train/good images.\n",
    "# - Prepares dev test DataLoader with labels (good=0, defect=1).\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import os\n",
    "\n",
    "# Transforms (ImageNet mean/std)\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "class ImagePathDataset(Dataset):\n",
    "    \"\"\"Minimal dataset returning (image_tensor, label).\n",
    "\n",
    "    Paths: list of filesystem paths; Labels: list[int] same length.\n",
    "    \"\"\"\n",
    "    def __init__(self, paths: List[Path], labels: List[int], transform=None):\n",
    "        self.paths = [Path(p) for p in paths]\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.paths[idx]\n",
    "        y = self.labels[idx]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n",
    "\n",
    "def _existing_category_root(category: str) -> Path:\n",
    "    \"\"\"Find an existing MVTec category root among common layouts.\n",
    "    Prefers MVTEC_ROOT, then 'datasets/MVTecAD', then 'MVtec_dataset'.\n",
    "    \"\"\"\n",
    "    candidates = [\n",
    "        MVTEC_ROOT / category,\n",
    "        Path(\"datasets/MVTecAD\") / category,\n",
    "        Path(\"MVtec_dataset\") / category,\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    raise FileNotFoundError(f\"MVTec category not found: {category}\")\n",
    "\n",
    "def _list_images(d: Path) -> List[Path]:\n",
    "    exts = {\".png\", \".jpg\", \".jpeg\"}\n",
    "    if not d.exists():\n",
    "        return []\n",
    "    return sorted([p for p in d.rglob('*') if p.suffix.lower() in exts])\n",
    "\n",
    "def build_cv_and_test_loaders(category: str, k_splits: int = 5, batch_size: int = 32) -> Tuple[list, DataLoader]:\n",
    "    \"\"\"Return (cv_folds, test_loader).\n",
    "\n",
    "    cv_folds: list of dicts with 'train_loader' and 'val_loader'.\n",
    "    test_loader: dev test DataLoader with labels (0=good,1=defect).\n",
    "    \"\"\"\n",
    "    root = _existing_category_root(category)\n",
    "    train_good = _list_images(root / 'train' / 'good')\n",
    "    assert len(train_good) > 0, f\"No train/good images found for {category}\"\n",
    "\n",
    "    # Prepare KFold over indices (all labels are 0 in train).\n",
    "    kf = KFold(n_splits=k_splits, shuffle=True, random_state=seeds[0] if seeds else 0)\n",
    "    base_ds = ImagePathDataset(train_good, [0] * len(train_good), transform=_transform)\n",
    "\n",
    "    num_workers = min(4, os.cpu_count() or 1)\n",
    "    cv_folds = []\n",
    "    for fold_id, (tr_idx, va_idx) in enumerate(kf.split(range(len(train_good)))):\n",
    "        tr_ds = Subset(base_ds, tr_idx)\n",
    "        va_ds = Subset(base_ds, va_idx)\n",
    "        tr_loader = DataLoader(tr_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "        va_loader = DataLoader(va_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "        cv_folds.append({\n",
    "            'fold': fold_id,\n",
    "            'train_loader': tr_loader,\n",
    "            'val_loader': va_loader,\n",
    "            'n_train': len(tr_idx),\n",
    "            'n_val': len(va_idx),\n",
    "        })\n",
    "\n",
    "    # Build dev test loader with labels (good=0, others=1).\n",
    "    test_dir = root / 'test'\n",
    "    good_paths = _list_images(test_dir / 'good')\n",
    "    defect_paths = []\n",
    "    for sub in test_dir.iterdir():\n",
    "        if sub.is_dir() and sub.name != 'good':\n",
    "            defect_paths.extend(_list_images(sub))\n",
    "    test_paths = good_paths + defect_paths\n",
    "    test_labels = [0] * len(good_paths) + [1] * len(defect_paths)\n",
    "    assert len(test_paths) > 0, f\"No test images found for {category}\"\n",
    "\n",
    "    test_ds = ImagePathDataset(test_paths, test_labels, transform=_transform)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return cv_folds, test_loader\n",
    "\n",
    "# Build loaders for the chosen dev category\n",
    "cv_folds, dev_test_loader = build_cv_and_test_loaders(dev_category, k_splits=5, batch_size=32)\n",
    "print(f\"[INFO] Category: {dev_category}\")\n",
    "print(f\"[INFO] CV folds: {len(cv_folds)}\")\n",
    "for f in cv_folds:\n",
    "    print(f\"  - fold {f['fold']}: n_train={f['n_train']}, n_val={f['n_val']}\")\n",
    "print(f\"[INFO] Dev test size: {len(dev_test_loader.dataset)} (good={sum(1 for y in dev_test_loader.dataset.labels if y==0)}, defect={sum(1 for y in dev_test_loader.dataset.labels if y==1)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7a81b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fold': 0,\n",
       "  'train_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796e2d8dd0>,\n",
       "  'val_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796f527a70>,\n",
       "  'n_train': 224,\n",
       "  'n_val': 56},\n",
       " {'fold': 1,\n",
       "  'train_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796d2197f0>,\n",
       "  'val_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796d219880>,\n",
       "  'n_train': 224,\n",
       "  'n_val': 56},\n",
       " {'fold': 2,\n",
       "  'train_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796d219970>,\n",
       "  'val_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796d219a00>,\n",
       "  'n_train': 224,\n",
       "  'n_val': 56},\n",
       " {'fold': 3,\n",
       "  'train_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796d219af0>,\n",
       "  'val_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796d219b80>,\n",
       "  'n_train': 224,\n",
       "  'n_val': 56},\n",
       " {'fold': 4,\n",
       "  'train_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796d219c70>,\n",
       "  'val_loader': <torch.utils.data.dataloader.DataLoader at 0x7e796d219d00>,\n",
       "  'n_train': 224,\n",
       "  'n_val': 56}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 閾値決定（FPR=1% 目標、dev の test のみ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: ここでは閾値決定の骨子のみを用意。\n",
    "# 実装時は dev の test スコア分布から FPR=1% となるスコアを求めてください。\n",
    "image_fpr_target = 0.01\n",
    "threshold_value = None  # TODO: compute from dev test scores\n",
    "threshold_source = f\"dev_{dev_category}_test\"\n",
    "print(\"[INFO] threshold_source:\", threshold_source)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 固定パイプラインの保存（assets/fixed_pipeline.json）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# デフォルトではファイルを書き出さない（テンプレートのため）。\n",
    "# 実際に保存したい場合は SAVE_FIXED=True にして実行してください。\n",
    "SAVE_FIXED = False\n",
    "\n",
    "fixed_pipeline = {\n",
    "    \"common\": {\"image_size\": image_size, \"seeds\": seeds},\n",
    "    \"threshold\": {\"image_fpr_target\": image_fpr_target, \"value\": threshold_value, \"source\": threshold_source},\n",
    "    \"mahalanobis\": {\"backbone\": backbone, \"cov_estimator\": cov_estimator},\n",
    "    \"padim\": {\"layers\": padim_layers, \"channel_subsample\": padim_channel_subsample}\n",
    "}\n",
    "\n",
    "assets_dir = Path(\"assets\")\n",
    "assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "cfg_path = assets_dir / \"fixed_pipeline.json\"\n",
    "\n",
    "if SAVE_FIXED:\n",
    "    with cfg_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(fixed_pipeline, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"[INFO] Saved: {cfg_path}\")\n",
    "else:\n",
    "    print(\"[INFO] SAVE_FIXED=False のためファイルは出力しません。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次の手順\n",
    "- 上記のテンプレート関数に実装を追加し、dev の test から閾値を決めて `SAVE_FIXED=True` で JSON を保存。\n",
    "- その後 `02_evaluation_report.ipynb` で eval カテゴリを一発評価。\n",
    "- リーク防止のため、02 ではパラメータ・閾値を変更しないこと。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-anomaly-detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
